{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asong11155/HyGNN/blob/main/HyGNN_DDI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIvYbPoP73FF",
        "outputId": "6ca45efc-3019-4e88-eb53-d47441c535a0",
        "jupyter": {
          "is_executing": true
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG9atGpX4RvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35360e87-3415-47bb-dc75-7380578c7ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/EDA.py#Dont worry, you dont': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python '/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/EDA.py'#Don't worry, you don't have to run this"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 2. 安装 PyTorch 2.2.1 (支持 CUDA 12.1)\n",
        "!pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# 3. 安装 DGL (GPU版本，配合 CUDA 12.1)\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/cu121/repo.html\n",
        "\n",
        "# 4. 安装 torchdata (修复 ModuleNotFoundError)\n",
        "!pip install torchdata==0.7.1\n",
        "\n",
        "# 5. 安装 Numpy 1.26.4 (修复 Numba 报错的关键！)\n",
        "!pip install numpy==1.26.4\n",
        "\n",
        "# 6. 强制安装 karateclub (跳过它对旧版 Numpy 的依赖检查)\n",
        "!pip install karateclub --no-deps\n",
        "# 手动补充 karateclub 需要的其他库\n",
        "!pip install python-Levenshtein gensim networkx scikit-learn tqdm\n",
        "\n",
        "# 7. 安装 node2vec\n",
        "!pip install node2vec\n",
        "\n",
        "print(\"\\n✅ 所有环境已准备就绪！请执行最后一步：重启会话！\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnaE-I1aUuH7",
        "outputId": "d1d7b31d-c9d3-4145-e903-c03923b9669f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement torch==2.2.1 (from versions: none)\n",
            "ERROR: No matching distribution found for torch==2.2.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in links: https://data.dgl.ai/wheels/cu121/repo.html\n",
            "Collecting dgl\n",
            "  Downloading dgl-0.1.3-py3-none-win_amd64.whl.metadata (471 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from dgl) (2.1.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from dgl) (1.15.3)\n",
            "Requirement already satisfied: networkx>=2.1 in e:\\programdata\\anaconda3\\lib\\site-packages (from dgl) (3.4.2)\n",
            "Downloading dgl-0.1.3-py3-none-win_amd64.whl (411 kB)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.1.3\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting torchdata==0.7.1\n",
            "  Downloading torchdata-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in e:\\programdata\\anaconda3\\lib\\site-packages (from torchdata==0.7.1) (2.3.0)\n",
            "Requirement already satisfied: requests in e:\\programdata\\anaconda3\\lib\\site-packages (from torchdata==0.7.1) (2.32.3)\n",
            "Collecting torch>=2 (from torchdata==0.7.1)\n",
            "  Downloading torch-2.9.1-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: filelock in e:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (4.12.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in e:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in e:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in e:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in e:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (2025.3.2)\n",
            "Requirement already satisfied: setuptools in e:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2->torchdata==0.7.1) (72.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2->torchdata==0.7.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=2->torchdata==0.7.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchdata==0.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in e:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchdata==0.7.1) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in e:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchdata==0.7.1) (2025.4.26)\n",
            "Downloading torchdata-0.7.1-py3-none-any.whl (184 kB)\n",
            "Downloading torch-2.9.1-cp313-cp313-win_amd64.whl (110.9 MB)\n",
            "   ---------------------------------------- 0.0/110.9 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.3/110.9 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.8/110.9 MB 2.2 MB/s eta 0:00:50\n",
            "   ---------------------------------------- 1.3/110.9 MB 2.4 MB/s eta 0:00:47\n",
            "    --------------------------------------- 1.6/110.9 MB 2.2 MB/s eta 0:00:50\n",
            "    --------------------------------------- 2.1/110.9 MB 2.3 MB/s eta 0:00:48\n",
            "   - -------------------------------------- 2.9/110.9 MB 2.4 MB/s eta 0:00:45\n",
            "   - -------------------------------------- 3.4/110.9 MB 2.5 MB/s eta 0:00:43\n",
            "   - -------------------------------------- 3.9/110.9 MB 2.5 MB/s eta 0:00:43\n",
            "   - -------------------------------------- 4.5/110.9 MB 2.5 MB/s eta 0:00:42\n",
            "   - -------------------------------------- 5.0/110.9 MB 2.5 MB/s eta 0:00:43\n",
            "   - -------------------------------------- 5.2/110.9 MB 2.5 MB/s eta 0:00:43\n",
            "   - -------------------------------------- 5.5/110.9 MB 2.4 MB/s eta 0:00:45\n",
            "   -- ------------------------------------- 5.8/110.9 MB 2.3 MB/s eta 0:00:47\n",
            "   -- ------------------------------------- 6.0/110.9 MB 2.3 MB/s eta 0:00:47\n",
            "   -- ------------------------------------- 6.6/110.9 MB 2.2 MB/s eta 0:00:48\n",
            "   -- ------------------------------------- 6.8/110.9 MB 2.2 MB/s eta 0:00:48\n",
            "   -- ------------------------------------- 7.1/110.9 MB 2.2 MB/s eta 0:00:49\n",
            "   -- ------------------------------------- 7.3/110.9 MB 2.1 MB/s eta 0:00:50\n",
            "   -- ------------------------------------- 7.6/110.9 MB 2.0 MB/s eta 0:00:51\n",
            "   -- ------------------------------------- 7.9/110.9 MB 2.0 MB/s eta 0:00:52\n",
            "   --- ------------------------------------ 8.4/110.9 MB 2.0 MB/s eta 0:00:52\n",
            "   --- ------------------------------------ 8.9/110.9 MB 2.0 MB/s eta 0:00:51\n",
            "   --- ------------------------------------ 9.4/110.9 MB 2.0 MB/s eta 0:00:50\n",
            "   --- ------------------------------------ 10.0/110.9 MB 2.1 MB/s eta 0:00:49\n",
            "   --- ------------------------------------ 10.5/110.9 MB 2.1 MB/s eta 0:00:49\n",
            "   --- ------------------------------------ 10.5/110.9 MB 2.1 MB/s eta 0:00:49\n",
            "   --- ------------------------------------ 10.7/110.9 MB 2.0 MB/s eta 0:00:50\n",
            "   --- ------------------------------------ 11.0/110.9 MB 2.0 MB/s eta 0:00:51\n",
            "   ---- ----------------------------------- 11.3/110.9 MB 1.9 MB/s eta 0:00:52\n",
            "   ---- ----------------------------------- 11.5/110.9 MB 1.9 MB/s eta 0:00:52\n",
            "   ---- ----------------------------------- 12.1/110.9 MB 1.9 MB/s eta 0:00:52\n",
            "   ---- ----------------------------------- 12.3/110.9 MB 1.9 MB/s eta 0:00:52\n",
            "   ---- ----------------------------------- 12.6/110.9 MB 1.9 MB/s eta 0:00:53\n",
            "   ---- ----------------------------------- 12.8/110.9 MB 1.9 MB/s eta 0:00:53\n",
            "   ---- ----------------------------------- 13.4/110.9 MB 1.9 MB/s eta 0:00:53\n",
            "   ---- ----------------------------------- 13.6/110.9 MB 1.9 MB/s eta 0:00:52\n",
            "   ----- ---------------------------------- 14.2/110.9 MB 1.9 MB/s eta 0:00:52\n",
            "   ----- ---------------------------------- 14.4/110.9 MB 1.9 MB/s eta 0:00:52\n",
            "   ----- ---------------------------------- 14.7/110.9 MB 1.9 MB/s eta 0:00:52\n",
            "   ----- ---------------------------------- 14.9/110.9 MB 1.8 MB/s eta 0:00:52\n",
            "   ----- ---------------------------------- 15.5/110.9 MB 1.8 MB/s eta 0:00:52\n",
            "   ----- ---------------------------------- 16.0/110.9 MB 1.9 MB/s eta 0:00:51\n",
            "   ----- ---------------------------------- 16.3/110.9 MB 1.9 MB/s eta 0:00:51\n",
            "   ----- ---------------------------------- 16.5/110.9 MB 1.9 MB/s eta 0:00:51\n",
            "   ------ --------------------------------- 16.8/110.9 MB 1.8 MB/s eta 0:00:51\n",
            "   ------ --------------------------------- 17.3/110.9 MB 1.8 MB/s eta 0:00:51\n",
            "   ------ --------------------------------- 17.8/110.9 MB 1.9 MB/s eta 0:00:50\n",
            "   ------ --------------------------------- 18.1/110.9 MB 1.9 MB/s eta 0:00:50\n",
            "   ------ --------------------------------- 18.1/110.9 MB 1.9 MB/s eta 0:00:50\n",
            "   ------ --------------------------------- 18.6/110.9 MB 1.8 MB/s eta 0:00:51\n",
            "   ------ --------------------------------- 19.1/110.9 MB 1.8 MB/s eta 0:00:51\n",
            "   ------ --------------------------------- 19.4/110.9 MB 1.8 MB/s eta 0:00:50\n",
            "   ------- -------------------------------- 19.9/110.9 MB 1.8 MB/s eta 0:00:50\n",
            "   ------- -------------------------------- 20.2/110.9 MB 1.8 MB/s eta 0:00:50\n",
            "   ------- -------------------------------- 20.4/110.9 MB 1.8 MB/s eta 0:00:50\n",
            "   ------- -------------------------------- 20.7/110.9 MB 1.8 MB/s eta 0:00:50\n",
            "   ------- -------------------------------- 21.0/110.9 MB 1.8 MB/s eta 0:00:50\n",
            "   ------- -------------------------------- 21.5/110.9 MB 1.8 MB/s eta 0:00:50\n",
            "   ------- -------------------------------- 22.0/110.9 MB 1.8 MB/s eta 0:00:49\n",
            "   -------- ------------------------------- 22.8/110.9 MB 1.9 MB/s eta 0:00:48\n",
            "   -------- ------------------------------- 23.3/110.9 MB 1.9 MB/s eta 0:00:47\n",
            "   -------- ------------------------------- 23.6/110.9 MB 1.9 MB/s eta 0:00:47\n",
            "   -------- ------------------------------- 23.9/110.9 MB 1.9 MB/s eta 0:00:47\n",
            "   -------- ------------------------------- 24.1/110.9 MB 1.9 MB/s eta 0:00:47\n",
            "   -------- ------------------------------- 24.6/110.9 MB 1.9 MB/s eta 0:00:47\n",
            "   --------- ------------------------------ 25.4/110.9 MB 1.9 MB/s eta 0:00:46\n",
            "   --------- ------------------------------ 26.0/110.9 MB 1.9 MB/s eta 0:00:45\n",
            "   --------- ------------------------------ 26.2/110.9 MB 1.9 MB/s eta 0:00:45\n",
            "   --------- ------------------------------ 27.0/110.9 MB 1.9 MB/s eta 0:00:44\n",
            "   --------- ------------------------------ 27.5/110.9 MB 1.9 MB/s eta 0:00:44\n",
            "   ---------- ----------------------------- 28.3/110.9 MB 1.9 MB/s eta 0:00:43\n",
            "   ---------- ----------------------------- 28.8/110.9 MB 2.0 MB/s eta 0:00:42\n",
            "   ---------- ----------------------------- 29.6/110.9 MB 2.0 MB/s eta 0:00:42\n",
            "   ---------- ----------------------------- 30.1/110.9 MB 2.0 MB/s eta 0:00:41\n",
            "   ----------- ---------------------------- 30.7/110.9 MB 2.0 MB/s eta 0:00:41\n",
            "   ----------- ---------------------------- 31.5/110.9 MB 2.0 MB/s eta 0:00:40\n",
            "   ----------- ---------------------------- 32.0/110.9 MB 2.0 MB/s eta 0:00:39\n",
            "   ----------- ---------------------------- 32.8/110.9 MB 2.0 MB/s eta 0:00:39\n",
            "   ----------- ---------------------------- 33.0/110.9 MB 2.0 MB/s eta 0:00:39\n",
            "   ------------ --------------------------- 33.8/110.9 MB 2.1 MB/s eta 0:00:38\n",
            "   ------------ --------------------------- 34.6/110.9 MB 2.1 MB/s eta 0:00:37\n",
            "   ------------ --------------------------- 35.1/110.9 MB 2.1 MB/s eta 0:00:37\n",
            "   ------------ --------------------------- 35.7/110.9 MB 2.1 MB/s eta 0:00:36\n",
            "   ------------- -------------------------- 36.4/110.9 MB 2.1 MB/s eta 0:00:36\n",
            "   ------------- -------------------------- 37.0/110.9 MB 2.1 MB/s eta 0:00:35\n",
            "   ------------- -------------------------- 37.5/110.9 MB 2.1 MB/s eta 0:00:35\n",
            "   ------------- -------------------------- 38.0/110.9 MB 2.1 MB/s eta 0:00:35\n",
            "   ------------- -------------------------- 38.0/110.9 MB 2.1 MB/s eta 0:00:35\n",
            "   ------------- -------------------------- 38.3/110.9 MB 2.1 MB/s eta 0:00:35\n",
            "   ------------- -------------------------- 38.8/110.9 MB 2.1 MB/s eta 0:00:35\n",
            "   -------------- ------------------------- 39.6/110.9 MB 2.1 MB/s eta 0:00:34\n",
            "   -------------- ------------------------- 39.8/110.9 MB 2.1 MB/s eta 0:00:34\n",
            "   -------------- ------------------------- 40.1/110.9 MB 2.1 MB/s eta 0:00:34\n",
            "   -------------- ------------------------- 40.4/110.9 MB 2.1 MB/s eta 0:00:34\n",
            "   -------------- ------------------------- 40.9/110.9 MB 2.1 MB/s eta 0:00:34\n",
            "   -------------- ------------------------- 41.2/110.9 MB 2.1 MB/s eta 0:00:34\n",
            "   --------------- ------------------------ 41.7/110.9 MB 2.1 MB/s eta 0:00:34\n",
            "   --------------- ------------------------ 42.2/110.9 MB 2.1 MB/s eta 0:00:33\n",
            "   --------------- ------------------------ 42.7/110.9 MB 2.1 MB/s eta 0:00:33\n",
            "   --------------- ------------------------ 43.3/110.9 MB 2.1 MB/s eta 0:00:33\n",
            "   --------------- ------------------------ 43.8/110.9 MB 2.1 MB/s eta 0:00:32\n",
            "   --------------- ------------------------ 44.3/110.9 MB 2.1 MB/s eta 0:00:32\n",
            "   ---------------- ----------------------- 44.8/110.9 MB 2.1 MB/s eta 0:00:32\n",
            "   ---------------- ----------------------- 45.4/110.9 MB 2.1 MB/s eta 0:00:31\n",
            "   ---------------- ----------------------- 45.9/110.9 MB 2.1 MB/s eta 0:00:31\n",
            "   ---------------- ----------------------- 46.4/110.9 MB 2.1 MB/s eta 0:00:31\n",
            "   ---------------- ----------------------- 46.9/110.9 MB 2.1 MB/s eta 0:00:30\n",
            "   ----------------- ---------------------- 47.4/110.9 MB 2.1 MB/s eta 0:00:30\n",
            "   ----------------- ---------------------- 48.0/110.9 MB 2.1 MB/s eta 0:00:30\n",
            "   ----------------- ---------------------- 48.5/110.9 MB 2.2 MB/s eta 0:00:30\n",
            "   ----------------- ---------------------- 49.0/110.9 MB 2.2 MB/s eta 0:00:29\n",
            "   ----------------- ---------------------- 49.5/110.9 MB 2.2 MB/s eta 0:00:29\n",
            "   ------------------ --------------------- 50.1/110.9 MB 2.2 MB/s eta 0:00:29\n",
            "   ------------------ --------------------- 50.6/110.9 MB 2.2 MB/s eta 0:00:28\n",
            "   ------------------ --------------------- 51.1/110.9 MB 2.2 MB/s eta 0:00:28\n",
            "   ------------------ --------------------- 51.9/110.9 MB 2.2 MB/s eta 0:00:28\n",
            "   ------------------ --------------------- 52.4/110.9 MB 2.2 MB/s eta 0:00:27\n",
            "   ------------------- -------------------- 53.0/110.9 MB 2.2 MB/s eta 0:00:27\n",
            "   ------------------- -------------------- 53.5/110.9 MB 2.2 MB/s eta 0:00:27\n",
            "   ------------------- -------------------- 54.0/110.9 MB 2.2 MB/s eta 0:00:26\n",
            "   ------------------- -------------------- 54.0/110.9 MB 2.2 MB/s eta 0:00:26\n",
            "   ------------------- -------------------- 54.0/110.9 MB 2.2 MB/s eta 0:00:26\n",
            "   ------------------- -------------------- 54.5/110.9 MB 2.2 MB/s eta 0:00:27\n",
            "   ------------------- -------------------- 55.1/110.9 MB 2.2 MB/s eta 0:00:26\n",
            "   -------------------- ------------------- 55.6/110.9 MB 2.2 MB/s eta 0:00:26\n",
            "   -------------------- ------------------- 56.4/110.9 MB 2.2 MB/s eta 0:00:26\n",
            "   -------------------- ------------------- 56.9/110.9 MB 2.2 MB/s eta 0:00:25\n",
            "   -------------------- ------------------- 57.1/110.9 MB 2.2 MB/s eta 0:00:25\n",
            "   -------------------- ------------------- 57.9/110.9 MB 2.2 MB/s eta 0:00:25\n",
            "   --------------------- ------------------ 58.5/110.9 MB 2.2 MB/s eta 0:00:24\n",
            "   --------------------- ------------------ 59.0/110.9 MB 2.2 MB/s eta 0:00:24\n",
            "   --------------------- ------------------ 59.5/110.9 MB 2.2 MB/s eta 0:00:24\n",
            "   --------------------- ------------------ 59.8/110.9 MB 2.2 MB/s eta 0:00:24\n",
            "   --------------------- ------------------ 60.3/110.9 MB 2.2 MB/s eta 0:00:24\n",
            "   --------------------- ------------------ 60.8/110.9 MB 2.2 MB/s eta 0:00:23\n",
            "   ---------------------- ----------------- 61.6/110.9 MB 2.2 MB/s eta 0:00:23\n",
            "   ---------------------- ----------------- 62.1/110.9 MB 2.2 MB/s eta 0:00:23\n",
            "   ---------------------- ----------------- 62.9/110.9 MB 2.2 MB/s eta 0:00:22\n",
            "   ---------------------- ----------------- 63.4/110.9 MB 2.2 MB/s eta 0:00:22\n",
            "   ----------------------- ---------------- 64.0/110.9 MB 2.2 MB/s eta 0:00:22\n",
            "   ----------------------- ---------------- 64.7/110.9 MB 2.2 MB/s eta 0:00:21\n",
            "   ----------------------- ---------------- 65.3/110.9 MB 2.2 MB/s eta 0:00:21\n",
            "   ----------------------- ---------------- 65.5/110.9 MB 2.2 MB/s eta 0:00:21\n",
            "   ----------------------- ---------------- 65.8/110.9 MB 2.2 MB/s eta 0:00:21\n",
            "   ----------------------- ---------------- 66.1/110.9 MB 2.2 MB/s eta 0:00:21\n",
            "   ----------------------- ---------------- 66.1/110.9 MB 2.2 MB/s eta 0:00:21\n",
            "   ------------------------ --------------- 66.6/110.9 MB 2.2 MB/s eta 0:00:21\n",
            "   ------------------------ --------------- 66.8/110.9 MB 2.2 MB/s eta 0:00:20\n",
            "   ------------------------ --------------- 67.4/110.9 MB 2.2 MB/s eta 0:00:20\n",
            "   ------------------------ --------------- 67.9/110.9 MB 2.2 MB/s eta 0:00:20\n",
            "   ------------------------ --------------- 68.4/110.9 MB 2.2 MB/s eta 0:00:20\n",
            "   ------------------------ --------------- 68.9/110.9 MB 2.2 MB/s eta 0:00:20\n",
            "   ------------------------- -------------- 69.5/110.9 MB 2.2 MB/s eta 0:00:19\n",
            "   ------------------------- -------------- 70.0/110.9 MB 2.2 MB/s eta 0:00:19\n",
            "   ------------------------- -------------- 70.5/110.9 MB 2.2 MB/s eta 0:00:19\n",
            "   ------------------------- -------------- 71.0/110.9 MB 2.2 MB/s eta 0:00:19\n",
            "   ------------------------- -------------- 71.8/110.9 MB 2.2 MB/s eta 0:00:18\n",
            "   -------------------------- ------------- 72.4/110.9 MB 2.2 MB/s eta 0:00:18\n",
            "   -------------------------- ------------- 72.9/110.9 MB 2.2 MB/s eta 0:00:18\n",
            "   -------------------------- ------------- 73.1/110.9 MB 2.2 MB/s eta 0:00:17\n",
            "   -------------------------- ------------- 73.7/110.9 MB 2.2 MB/s eta 0:00:17\n",
            "   -------------------------- ------------- 73.9/110.9 MB 2.2 MB/s eta 0:00:17\n",
            "   -------------------------- ------------- 74.4/110.9 MB 2.2 MB/s eta 0:00:17\n",
            "   --------------------------- ------------ 75.0/110.9 MB 2.3 MB/s eta 0:00:16\n",
            "   --------------------------- ------------ 75.5/110.9 MB 2.3 MB/s eta 0:00:16\n",
            "   --------------------------- ------------ 76.0/110.9 MB 2.3 MB/s eta 0:00:16\n",
            "   --------------------------- ------------ 76.5/110.9 MB 2.3 MB/s eta 0:00:16\n",
            "   --------------------------- ------------ 76.8/110.9 MB 2.3 MB/s eta 0:00:16\n",
            "   --------------------------- ------------ 77.3/110.9 MB 2.3 MB/s eta 0:00:15\n",
            "   ---------------------------- ----------- 77.9/110.9 MB 2.3 MB/s eta 0:00:15\n",
            "   ---------------------------- ----------- 78.1/110.9 MB 2.3 MB/s eta 0:00:15\n",
            "   ---------------------------- ----------- 78.1/110.9 MB 2.3 MB/s eta 0:00:15\n",
            "   ---------------------------- ----------- 78.4/110.9 MB 2.3 MB/s eta 0:00:15\n",
            "   ---------------------------- ----------- 78.4/110.9 MB 2.3 MB/s eta 0:00:15\n",
            "   ---------------------------- ----------- 78.9/110.9 MB 2.3 MB/s eta 0:00:15\n",
            "   ---------------------------- ----------- 79.2/110.9 MB 2.3 MB/s eta 0:00:15\n",
            "   ---------------------------- ----------- 79.7/110.9 MB 2.3 MB/s eta 0:00:14\n",
            "   ---------------------------- ----------- 80.0/110.9 MB 2.3 MB/s eta 0:00:14\n",
            "   ----------------------------- ---------- 80.5/110.9 MB 2.3 MB/s eta 0:00:14\n",
            "   ----------------------------- ---------- 80.7/110.9 MB 2.3 MB/s eta 0:00:14\n",
            "   ----------------------------- ---------- 81.3/110.9 MB 2.3 MB/s eta 0:00:14\n",
            "   ----------------------------- ---------- 81.3/110.9 MB 2.3 MB/s eta 0:00:14\n",
            "   ----------------------------- ---------- 81.8/110.9 MB 2.3 MB/s eta 0:00:13\n",
            "   ----------------------------- ---------- 82.3/110.9 MB 2.3 MB/s eta 0:00:13\n",
            "   ----------------------------- ---------- 82.8/110.9 MB 2.3 MB/s eta 0:00:13\n",
            "   ----------------------------- ---------- 83.1/110.9 MB 2.3 MB/s eta 0:00:13\n",
            "   ------------------------------ --------- 83.4/110.9 MB 2.3 MB/s eta 0:00:13\n",
            "   ------------------------------ --------- 83.6/110.9 MB 2.3 MB/s eta 0:00:13\n",
            "   ------------------------------ --------- 83.9/110.9 MB 2.3 MB/s eta 0:00:12\n",
            "   ------------------------------ --------- 84.1/110.9 MB 2.3 MB/s eta 0:00:12\n",
            "   ------------------------------ --------- 84.4/110.9 MB 2.3 MB/s eta 0:00:12\n",
            "   ------------------------------ --------- 84.9/110.9 MB 2.3 MB/s eta 0:00:12\n",
            "   ------------------------------ --------- 85.5/110.9 MB 2.3 MB/s eta 0:00:12\n",
            "   ------------------------------- -------- 86.0/110.9 MB 2.3 MB/s eta 0:00:11\n",
            "   ------------------------------- -------- 86.5/110.9 MB 2.3 MB/s eta 0:00:11\n",
            "   ------------------------------- -------- 87.0/110.9 MB 2.3 MB/s eta 0:00:11\n",
            "   ------------------------------- -------- 87.3/110.9 MB 2.3 MB/s eta 0:00:11\n",
            "   ------------------------------- -------- 87.8/110.9 MB 2.3 MB/s eta 0:00:11\n",
            "   ------------------------------- -------- 88.1/110.9 MB 2.3 MB/s eta 0:00:11\n",
            "   ------------------------------- -------- 88.3/110.9 MB 2.3 MB/s eta 0:00:10\n",
            "   ------------------------------- -------- 88.6/110.9 MB 2.3 MB/s eta 0:00:10\n",
            "   -------------------------------- ------- 88.9/110.9 MB 2.3 MB/s eta 0:00:10\n",
            "   -------------------------------- ------- 89.4/110.9 MB 2.3 MB/s eta 0:00:10\n",
            "   -------------------------------- ------- 89.7/110.9 MB 2.3 MB/s eta 0:00:10\n",
            "   -------------------------------- ------- 89.9/110.9 MB 2.3 MB/s eta 0:00:10\n",
            "   -------------------------------- ------- 90.2/110.9 MB 2.3 MB/s eta 0:00:10\n",
            "   -------------------------------- ------- 90.4/110.9 MB 2.2 MB/s eta 0:00:10\n",
            "   -------------------------------- ------- 90.4/110.9 MB 2.2 MB/s eta 0:00:10\n",
            "   -------------------------------- ------- 90.7/110.9 MB 2.2 MB/s eta 0:00:10\n",
            "   -------------------------------- ------- 91.0/110.9 MB 2.2 MB/s eta 0:00:09\n",
            "   -------------------------------- ------- 91.2/110.9 MB 2.2 MB/s eta 0:00:09\n",
            "   -------------------------------- ------- 91.5/110.9 MB 2.2 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 91.8/110.9 MB 2.2 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 92.0/110.9 MB 2.2 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 92.3/110.9 MB 2.2 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 92.5/110.9 MB 2.2 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 92.5/110.9 MB 2.2 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 92.8/110.9 MB 2.1 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 93.1/110.9 MB 2.1 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 93.1/110.9 MB 2.1 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 93.3/110.9 MB 2.1 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 93.3/110.9 MB 2.1 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 93.6/110.9 MB 2.1 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 93.8/110.9 MB 2.0 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 93.8/110.9 MB 2.0 MB/s eta 0:00:09\n",
            "   --------------------------------- ------ 93.8/110.9 MB 2.0 MB/s eta 0:00:09\n",
            "   ---------------------------------- ----- 94.4/110.9 MB 2.0 MB/s eta 0:00:09\n",
            "   ---------------------------------- ----- 94.6/110.9 MB 2.0 MB/s eta 0:00:09\n",
            "   ---------------------------------- ----- 94.9/110.9 MB 2.0 MB/s eta 0:00:09\n",
            "   ---------------------------------- ----- 95.2/110.9 MB 2.0 MB/s eta 0:00:09\n",
            "   ---------------------------------- ----- 95.4/110.9 MB 1.9 MB/s eta 0:00:08\n",
            "   ---------------------------------- ----- 95.4/110.9 MB 1.9 MB/s eta 0:00:08\n",
            "   ---------------------------------- ----- 95.7/110.9 MB 1.9 MB/s eta 0:00:08\n",
            "   ---------------------------------- ----- 95.9/110.9 MB 1.9 MB/s eta 0:00:08\n",
            "   ---------------------------------- ----- 96.2/110.9 MB 1.9 MB/s eta 0:00:08\n",
            "   ---------------------------------- ----- 96.5/110.9 MB 1.9 MB/s eta 0:00:08\n",
            "   ---------------------------------- ----- 96.5/110.9 MB 1.9 MB/s eta 0:00:08\n",
            "   ---------------------------------- ----- 96.7/110.9 MB 1.9 MB/s eta 0:00:08\n",
            "   ---------------------------------- ----- 97.0/110.9 MB 1.9 MB/s eta 0:00:08\n",
            "   ----------------------------------- ---- 97.3/110.9 MB 1.9 MB/s eta 0:00:08\n",
            "   ----------------------------------- ---- 97.5/110.9 MB 1.9 MB/s eta 0:00:08\n",
            "   ----------------------------------- ---- 97.8/110.9 MB 1.9 MB/s eta 0:00:07\n",
            "   ----------------------------------- ---- 98.3/110.9 MB 1.9 MB/s eta 0:00:07\n",
            "   ----------------------------------- ---- 98.6/110.9 MB 1.9 MB/s eta 0:00:07\n",
            "   ----------------------------------- ---- 98.8/110.9 MB 1.9 MB/s eta 0:00:07\n",
            "   ----------------------------------- ---- 99.4/110.9 MB 1.9 MB/s eta 0:00:07\n",
            "   ----------------------------------- ---- 99.6/110.9 MB 1.9 MB/s eta 0:00:07\n",
            "   ------------------------------------ --- 99.9/110.9 MB 1.9 MB/s eta 0:00:06\n",
            "   ------------------------------------ --- 100.4/110.9 MB 1.8 MB/s eta 0:00:06\n",
            "   ------------------------------------ --- 100.4/110.9 MB 1.8 MB/s eta 0:00:06\n",
            "   ------------------------------------ --- 100.9/110.9 MB 1.8 MB/s eta 0:00:06\n",
            "   ------------------------------------ --- 101.2/110.9 MB 1.8 MB/s eta 0:00:06\n",
            "   ------------------------------------ --- 101.7/110.9 MB 1.8 MB/s eta 0:00:06\n",
            "   ------------------------------------ --- 102.2/110.9 MB 1.8 MB/s eta 0:00:05\n",
            "   ------------------------------------ --- 102.5/110.9 MB 1.8 MB/s eta 0:00:05\n",
            "   ------------------------------------ --- 102.5/110.9 MB 1.8 MB/s eta 0:00:05\n",
            "   ------------------------------------- -- 102.8/110.9 MB 1.8 MB/s eta 0:00:05\n",
            "   ------------------------------------- -- 102.8/110.9 MB 1.8 MB/s eta 0:00:05\n",
            "   ------------------------------------- -- 103.0/110.9 MB 1.8 MB/s eta 0:00:05\n",
            "   ------------------------------------- -- 103.3/110.9 MB 1.7 MB/s eta 0:00:05\n",
            "   ------------------------------------- -- 103.3/110.9 MB 1.7 MB/s eta 0:00:05\n",
            "   ------------------------------------- -- 103.8/110.9 MB 1.7 MB/s eta 0:00:05\n",
            "   ------------------------------------- -- 104.3/110.9 MB 1.7 MB/s eta 0:00:04\n",
            "   ------------------------------------- -- 104.6/110.9 MB 1.7 MB/s eta 0:00:04\n",
            "   ------------------------------------- -- 104.9/110.9 MB 1.7 MB/s eta 0:00:04\n",
            "   ------------------------------------- -- 105.4/110.9 MB 1.7 MB/s eta 0:00:04\n",
            "   -------------------------------------- - 105.6/110.9 MB 1.7 MB/s eta 0:00:04\n",
            "   -------------------------------------- - 106.2/110.9 MB 1.7 MB/s eta 0:00:03\n",
            "   -------------------------------------- - 106.7/110.9 MB 1.7 MB/s eta 0:00:03\n",
            "   -------------------------------------- - 107.0/110.9 MB 1.7 MB/s eta 0:00:03\n",
            "   -------------------------------------- - 107.5/110.9 MB 1.7 MB/s eta 0:00:03\n",
            "   -------------------------------------- - 107.7/110.9 MB 1.7 MB/s eta 0:00:02\n",
            "   ---------------------------------------  108.3/110.9 MB 1.7 MB/s eta 0:00:02\n",
            "   ---------------------------------------  108.8/110.9 MB 1.7 MB/s eta 0:00:02\n",
            "   ---------------------------------------  109.3/110.9 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  109.8/110.9 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  110.1/110.9 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  110.1/110.9 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  110.6/110.9 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  110.9/110.9 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 110.9/110.9 MB 1.7 MB/s eta 0:00:00\n",
            "Installing collected packages: torch, torchdata\n",
            "\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   ---------------------------------------- 0/2 [torch]\n",
            "   -------------------- ------------------- 1/2 [torchdata]\n",
            "   -------------------- ------------------- 1/2 [torchdata]\n",
            "   -------------------- ------------------- 1/2 [torchdata]\n",
            "   ---------------------------------------- 2/2 [torchdata]\n",
            "\n",
            "Successfully installed torch-2.9.1 torchdata-0.7.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\asong.PC-20240204RTHA\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4.tar.gz (15.8 MB)\n",
            "     ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.3/15.8 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.5/15.8 MB 1.9 MB/s eta 0:00:08\n",
            "     --- ------------------------------------ 1.3/15.8 MB 2.3 MB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 1.8/15.8 MB 2.6 MB/s eta 0:00:06\n",
            "     ----- ---------------------------------- 2.4/15.8 MB 2.8 MB/s eta 0:00:05\n",
            "     ------- -------------------------------- 3.1/15.8 MB 2.7 MB/s eta 0:00:05\n",
            "     --------- ------------------------------ 3.7/15.8 MB 2.7 MB/s eta 0:00:05\n",
            "     ----------- ---------------------------- 4.5/15.8 MB 2.8 MB/s eta 0:00:05\n",
            "     ------------ --------------------------- 5.0/15.8 MB 2.8 MB/s eta 0:00:04\n",
            "     -------------- ------------------------- 5.8/15.8 MB 2.9 MB/s eta 0:00:04\n",
            "     --------------- ------------------------ 6.3/15.8 MB 2.9 MB/s eta 0:00:04\n",
            "     ----------------- ---------------------- 6.8/15.8 MB 2.8 MB/s eta 0:00:04\n",
            "     ------------------ --------------------- 7.3/15.8 MB 2.8 MB/s eta 0:00:03\n",
            "     ------------------- -------------------- 7.9/15.8 MB 2.8 MB/s eta 0:00:03\n",
            "     --------------------- ------------------ 8.4/15.8 MB 2.8 MB/s eta 0:00:03\n",
            "     --------------------- ------------------ 8.7/15.8 MB 2.7 MB/s eta 0:00:03\n",
            "     ---------------------- ----------------- 8.9/15.8 MB 2.6 MB/s eta 0:00:03\n",
            "     ----------------------- ---------------- 9.2/15.8 MB 2.6 MB/s eta 0:00:03\n",
            "     ------------------------ --------------- 9.7/15.8 MB 2.5 MB/s eta 0:00:03\n",
            "     ------------------------- -------------- 10.2/15.8 MB 2.5 MB/s eta 0:00:03\n",
            "     -------------------------- ------------- 10.5/15.8 MB 2.4 MB/s eta 0:00:03\n",
            "     --------------------------- ------------ 10.7/15.8 MB 2.4 MB/s eta 0:00:03\n",
            "     ---------------------------- ----------- 11.3/15.8 MB 2.4 MB/s eta 0:00:02\n",
            "     ------------------------------ --------- 12.1/15.8 MB 2.5 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 12.3/15.8 MB 2.4 MB/s eta 0:00:02\n",
            "     -------------------------------- ------- 12.8/15.8 MB 2.4 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 13.4/15.8 MB 2.4 MB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 13.6/15.8 MB 2.4 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 14.2/15.8 MB 2.4 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 14.4/15.8 MB 2.4 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 14.9/15.8 MB 2.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------  15.5/15.8 MB 2.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 15.8/15.8 MB 2.4 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  Preparing metadata (pyproject.toml) did not run successfully.\n",
            "  exit code: 1\n",
            "  \n",
            "  [21 lines of output]\n",
            "  + E:\\ProgramData\\anaconda3\\python.exe C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-2bobnal3\\numpy_30c14fade04140e9b6f4e1c498a56304\\vendored-meson\\meson\\meson.py setup C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-2bobnal3\\numpy_30c14fade04140e9b6f4e1c498a56304 C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-2bobnal3\\numpy_30c14fade04140e9b6f4e1c498a56304\\.mesonpy-o2f8lzow -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-2bobnal3\\numpy_30c14fade04140e9b6f4e1c498a56304\\.mesonpy-o2f8lzow\\meson-python-native-file.ini\n",
            "  The Meson build system\n",
            "  Version: 1.2.99\n",
            "  Source dir: C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-2bobnal3\\numpy_30c14fade04140e9b6f4e1c498a56304\n",
            "  Build dir: C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-2bobnal3\\numpy_30c14fade04140e9b6f4e1c498a56304\\.mesonpy-o2f8lzow\n",
            "  Build type: native build\n",
            "  Project name: NumPy\n",
            "  Project version: 1.26.4\n",
            "  WARNING: Failed to activate VS environment: Could not parse vswhere.exe output\n",
            "  \n",
            "  ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
            "  The following exception(s) were encountered:\n",
            "  Running `icl \"\"` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  Running `cl /?` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  Running `cc --version` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  Running `gcc --version` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  Running `clang --version` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  Running `clang-cl /?` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  Running `pgcc --version` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  \n",
            "  A full log can be found at C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-2bobnal3\\numpy_30c14fade04140e9b6f4e1c498a56304\\.mesonpy-o2f8lzow\\meson-logs\\meson-log.txt\n",
            "  [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "Encountered error while generating package metadata.\n",
            "\n",
            "See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting karateclub\n",
            "  Downloading karateclub-1.3.3.tar.gz (64 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: karateclub\n",
            "  Building wheel for karateclub (setup.py): started\n",
            "  Building wheel for karateclub (setup.py): finished with status 'done'\n",
            "  Created wheel for karateclub: filename=karateclub-1.3.3-py3-none-any.whl size=101989 sha256=46f771602ae600739bf5519dcde4929d0bc59870df3468cac21b4cb92931621b\n",
            "  Stored in directory: c:\\users\\asong.pc-20240204rtha\\appdata\\local\\pip\\cache\\wheels\\ca\\e2\\0b\\cb369543bcf6af0b3f013493576c368ec0106daa270fa74bcb\n",
            "Successfully built karateclub\n",
            "Installing collected packages: karateclub\n",
            "Successfully installed karateclub-1.3.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  DEPRECATION: Building 'karateclub' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'karateclub'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "karateclub 1.3.3 requires python-louvain, which is not installed.\n",
            "karateclub 1.3.3 requires decorator==4.4.2, but you have decorator 5.1.1 which is incompatible.\n",
            "karateclub 1.3.3 requires networkx<2.7, but you have networkx 3.4.2 which is incompatible.\n",
            "karateclub 1.3.3 requires numpy<1.23.0, but you have numpy 2.1.3 which is incompatible.\n",
            "karateclub 1.3.3 requires pandas<=1.3.5, but you have pandas 2.2.3 which is incompatible.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: networkx in e:\\programdata\\anaconda3\\lib\\site-packages (3.4.2)\n",
            "Requirement already satisfied: scikit-learn in e:\\programdata\\anaconda3\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in e:\\programdata\\anaconda3\\lib\\site-packages (4.67.1)\n",
            "Collecting Levenshtein==0.27.3 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.3-cp313-cp313-win_amd64.whl.metadata (3.7 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.14.3-cp313-cp313-win_amd64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in e:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (2.1.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.15.3)\n",
            "Collecting smart_open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: colorama in e:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: wrapt in e:\\programdata\\anaconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n",
            "Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
            "Downloading levenshtein-0.27.3-cp313-cp313-win_amd64.whl (94 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp313-cp313-win_amd64.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
            "   ------------- -------------------------- 0.5/1.5 MB 1.6 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 0.8/1.5 MB 1.7 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 1.0/1.5 MB 1.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.5/1.5 MB 1.6 MB/s eta 0:00:00\n",
            "Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl (24.4 MB)\n",
            "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.3/24.4 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.8/24.4 MB 2.4 MB/s eta 0:00:10\n",
            "   - -------------------------------------- 1.0/24.4 MB 2.5 MB/s eta 0:00:10\n",
            "   -- ------------------------------------- 1.6/24.4 MB 2.4 MB/s eta 0:00:10\n",
            "   --- ------------------------------------ 2.1/24.4 MB 2.3 MB/s eta 0:00:10\n",
            "   ---- ----------------------------------- 2.6/24.4 MB 2.3 MB/s eta 0:00:10\n",
            "   ---- ----------------------------------- 2.9/24.4 MB 2.2 MB/s eta 0:00:10\n",
            "   ----- ---------------------------------- 3.4/24.4 MB 2.2 MB/s eta 0:00:10\n",
            "   ------ --------------------------------- 3.9/24.4 MB 2.2 MB/s eta 0:00:10\n",
            "   ------ --------------------------------- 4.2/24.4 MB 2.2 MB/s eta 0:00:10\n",
            "   -------- ------------------------------- 5.0/24.4 MB 2.3 MB/s eta 0:00:09\n",
            "   --------- ------------------------------ 5.5/24.4 MB 2.3 MB/s eta 0:00:09\n",
            "   --------- ------------------------------ 5.8/24.4 MB 2.3 MB/s eta 0:00:09\n",
            "   --------- ------------------------------ 6.0/24.4 MB 2.3 MB/s eta 0:00:09\n",
            "   ---------- ----------------------------- 6.6/24.4 MB 2.2 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 6.8/24.4 MB 2.2 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 7.1/24.4 MB 2.2 MB/s eta 0:00:08\n",
            "   ------------ --------------------------- 7.3/24.4 MB 2.1 MB/s eta 0:00:08\n",
            "   ------------ --------------------------- 7.9/24.4 MB 2.1 MB/s eta 0:00:08\n",
            "   ------------- -------------------------- 8.4/24.4 MB 2.1 MB/s eta 0:00:08\n",
            "   -------------- ------------------------- 8.7/24.4 MB 2.1 MB/s eta 0:00:08\n",
            "   --------------- ------------------------ 9.2/24.4 MB 2.1 MB/s eta 0:00:08\n",
            "   --------------- ------------------------ 9.7/24.4 MB 2.1 MB/s eta 0:00:07\n",
            "   ---------------- ----------------------- 10.0/24.4 MB 2.1 MB/s eta 0:00:07\n",
            "   ----------------- ---------------------- 10.5/24.4 MB 2.1 MB/s eta 0:00:07\n",
            "   ------------------ --------------------- 11.0/24.4 MB 2.1 MB/s eta 0:00:07\n",
            "   ------------------ --------------------- 11.3/24.4 MB 2.1 MB/s eta 0:00:07\n",
            "   ------------------- -------------------- 11.8/24.4 MB 2.1 MB/s eta 0:00:07\n",
            "   ------------------- -------------------- 12.1/24.4 MB 2.1 MB/s eta 0:00:06\n",
            "   -------------------- ------------------- 12.3/24.4 MB 2.1 MB/s eta 0:00:06\n",
            "   -------------------- ------------------- 12.6/24.4 MB 2.0 MB/s eta 0:00:06\n",
            "   -------------------- ------------------- 12.6/24.4 MB 2.0 MB/s eta 0:00:06\n",
            "   --------------------- ------------------ 12.8/24.4 MB 2.0 MB/s eta 0:00:06\n",
            "   --------------------- ------------------ 13.1/24.4 MB 1.9 MB/s eta 0:00:06\n",
            "   --------------------- ------------------ 13.4/24.4 MB 1.9 MB/s eta 0:00:06\n",
            "   ---------------------- ----------------- 13.6/24.4 MB 1.9 MB/s eta 0:00:06\n",
            "   ----------------------- ---------------- 14.2/24.4 MB 1.9 MB/s eta 0:00:06\n",
            "   ----------------------- ---------------- 14.4/24.4 MB 1.9 MB/s eta 0:00:06\n",
            "   ------------------------ --------------- 14.7/24.4 MB 1.9 MB/s eta 0:00:06\n",
            "   ------------------------ --------------- 15.2/24.4 MB 1.9 MB/s eta 0:00:05\n",
            "   ------------------------- -------------- 15.7/24.4 MB 1.9 MB/s eta 0:00:05\n",
            "   -------------------------- ------------- 16.0/24.4 MB 1.9 MB/s eta 0:00:05\n",
            "   -------------------------- ------------- 16.0/24.4 MB 1.9 MB/s eta 0:00:05\n",
            "   -------------------------- ------------- 16.3/24.4 MB 1.8 MB/s eta 0:00:05\n",
            "   --------------------------- ------------ 16.5/24.4 MB 1.8 MB/s eta 0:00:05\n",
            "   --------------------------- ------------ 17.0/24.4 MB 1.8 MB/s eta 0:00:05\n",
            "   ---------------------------- ----------- 17.6/24.4 MB 1.8 MB/s eta 0:00:04\n",
            "   ----------------------------- ---------- 18.1/24.4 MB 1.9 MB/s eta 0:00:04\n",
            "   ------------------------------ --------- 18.6/24.4 MB 1.9 MB/s eta 0:00:04\n",
            "   ------------------------------- -------- 19.1/24.4 MB 1.9 MB/s eta 0:00:03\n",
            "   -------------------------------- ------- 19.7/24.4 MB 1.9 MB/s eta 0:00:03\n",
            "   -------------------------------- ------- 19.9/24.4 MB 1.9 MB/s eta 0:00:03\n",
            "   --------------------------------- ------ 20.4/24.4 MB 1.9 MB/s eta 0:00:03\n",
            "   ---------------------------------- ----- 21.0/24.4 MB 1.9 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 21.5/24.4 MB 1.9 MB/s eta 0:00:02\n",
            "   ------------------------------------ --- 22.0/24.4 MB 1.9 MB/s eta 0:00:02\n",
            "   ------------------------------------ --- 22.5/24.4 MB 2.0 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 23.1/24.4 MB 2.0 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 23.3/24.4 MB 1.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  23.9/24.4 MB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 24.4/24.4 MB 2.0 MB/s eta 0:00:00\n",
            "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
            "Installing collected packages: smart_open, rapidfuzz, Levenshtein, gensim, python-Levenshtein\n",
            "\n",
            "   -------- ------------------------------- 1/5 [rapidfuzz]\n",
            "   ------------------------ --------------- 3/5 [gensim]\n",
            "   ------------------------ --------------- 3/5 [gensim]\n",
            "   ------------------------ --------------- 3/5 [gensim]\n",
            "   ------------------------ --------------- 3/5 [gensim]\n",
            "   ------------------------ --------------- 3/5 [gensim]\n",
            "   ------------------------ --------------- 3/5 [gensim]\n",
            "   ------------------------ --------------- 3/5 [gensim]\n",
            "   ---------------------------------------- 5/5 [python-Levenshtein]\n",
            "\n",
            "Successfully installed Levenshtein-0.27.3 gensim-4.4.0 python-Levenshtein-0.27.3 rapidfuzz-3.14.3 smart_open-7.5.0\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "✅ 所有环境已准备就绪！请执行最后一步：重启会话！\n",
            "\n",
            "Collecting node2vec\n",
            "  Downloading node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in c:\\users\\asong.pc-20240204rtha\\appdata\\roaming\\python\\python313\\site-packages (from node2vec) (4.4.0)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from node2vec) (1.4.2)\n",
            "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in e:\\programdata\\anaconda3\\lib\\site-packages (from node2vec) (3.4.2)\n",
            "Collecting numpy<2.0.0,>=1.24.0 (from node2vec)\n",
            "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  Preparing metadata (pyproject.toml) did not run successfully.\n",
            "  exit code: 1\n",
            "  \n",
            "  [21 lines of output]\n",
            "  + E:\\ProgramData\\anaconda3\\python.exe C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-xq6dtuiz\\numpy_79a4c9c2cb2847d9ab8b1f2613c64c27\\vendored-meson\\meson\\meson.py setup C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-xq6dtuiz\\numpy_79a4c9c2cb2847d9ab8b1f2613c64c27 C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-xq6dtuiz\\numpy_79a4c9c2cb2847d9ab8b1f2613c64c27\\.mesonpy-xwzkha_h -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-xq6dtuiz\\numpy_79a4c9c2cb2847d9ab8b1f2613c64c27\\.mesonpy-xwzkha_h\\meson-python-native-file.ini\n",
            "  The Meson build system\n",
            "  Version: 1.2.99\n",
            "  Source dir: C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-xq6dtuiz\\numpy_79a4c9c2cb2847d9ab8b1f2613c64c27\n",
            "  Build dir: C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-xq6dtuiz\\numpy_79a4c9c2cb2847d9ab8b1f2613c64c27\\.mesonpy-xwzkha_h\n",
            "  Build type: native build\n",
            "  Project name: NumPy\n",
            "  Project version: 1.26.4\n",
            "  WARNING: Failed to activate VS environment: Could not parse vswhere.exe output\n",
            "  \n",
            "  ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
            "  The following exception(s) were encountered:\n",
            "  Running `icl \"\"` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  Running `cl /?` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  Running `cc --version` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  Running `gcc --version` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  Running `clang --version` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  Running `clang-cl /?` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  Running `pgcc --version` gave \"[WinError 2] 系统找不到指定的文件。\"\n",
            "  \n",
            "  A full log can be found at C:\\Users\\asong.PC-20240204RTHA\\AppData\\Local\\Temp\\pip-install-xq6dtuiz\\numpy_79a4c9c2cb2847d9ab8b1f2613c64c27\\.mesonpy-xwzkha_h\\meson-logs\\meson-log.txt\n",
            "  [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "Encountered error while generating package metadata.\n",
            "\n",
            "See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 安装 PyTorch 2.2.1 (CPU 版本)\n",
        "# 关键修改：将 index-url 改为 cpu 专用链接\n",
        "!pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "# 3. 安装 DGL (CPU 版本，配合 PyTorch 2.2)\n",
        "# 关键修改：链接改为 dgl 针对 torch-2.2 的专用仓库，且不带 cuXX 后缀\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/torch-2.2/repo.html\n",
        "\n",
        "# 4. 安装 torchdata (保持不变)\n",
        "!pip install torchdata==0.7.1\n",
        "\n",
        "# 5. 安装 Numpy 1.26.4 (保持不变，这是为了兼容性)\n",
        "!pip install numpy==1.26.4\n",
        "\n",
        "# 6. 强制安装 karateclub (保持不变，跳过依赖检查以保护 Numpy 版本)\n",
        "!pip install karateclub --no-deps\n",
        "# 手动补充 karateclub 需要的其他库\n",
        "!pip install python-Levenshtein gensim networkx scikit-learn tqdm\n",
        "\n",
        "# 7. 安装 node2vec (保持不变)\n",
        "!pip install node2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zJgS4KYcg4XQ",
        "outputId": "ffbfcbf1-ebf1-4204-ca04-c5ad45e40bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Collecting torch==2.2.1\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.1%2Bcpu-cp312-cp312-linux_x86_64.whl (186.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.7/186.7 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.17.1\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.17.1%2Bcpu-cp312-cp312-linux_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.2.1\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.2.1%2Bcpu-cp312-cp312-linux_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.1) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.1) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.1) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.1) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.1) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.1) (1.3.0)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.24.0+cu126\n",
            "    Uninstalling torchvision-0.24.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.24.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.9.0+cu126\n",
            "    Uninstalling torchaudio-2.9.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "Successfully installed torch-2.2.1+cpu torchaudio-2.2.1+cpu torchvision-0.17.1+cpu\n",
            "Looking in links: https://data.dgl.ai/wheels/torch-2.2/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/torch-2.2/dgl-2.4.0-cp312-cp312-manylinux1_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.12/dist-packages (from dgl) (3.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from dgl) (25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dgl) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from dgl) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from dgl) (4.67.1)\n",
            "Requirement already satisfied: torch<=2.4.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.2.1+cpu)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (3.20.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<=2.4.0->dgl) (2025.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.4.0->dgl) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch<=2.4.0->dgl) (1.3.0)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-2.4.0\n",
            "Collecting torchdata==0.7.1\n",
            "  Downloading torchdata-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata==0.7.1) (2.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchdata==0.7.1) (2.32.4)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.12/dist-packages (from torchdata==0.7.1) (2.2.1+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.7.1) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.7.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.7.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.7.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.7.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchdata==0.7.1) (2025.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.7.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.7.1) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.7.1) (2025.11.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2->torchdata==0.7.1) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=2->torchdata==0.7.1) (1.3.0)\n",
            "Downloading torchdata-0.7.1-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/184.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchdata\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.11.0\n",
            "    Uninstalling torchdata-0.11.0:\n",
            "      Successfully uninstalled torchdata-0.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torchdata-0.7.1\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "da49424a59374c00a64823cadacb839f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting karateclub\n",
            "  Downloading karateclub-1.3.3.tar.gz (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: karateclub\n",
            "  Building wheel for karateclub (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for karateclub: filename=karateclub-1.3.3-py3-none-any.whl size=101979 sha256=f38a6ede78d0ee187c367304d20fabb4648624046e5dcb4543f7f6b39895b6b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/59/5b/cec587a448c281393eeed3604826bc3e3460970d69b23f7fe4\n",
            "Successfully built karateclub\n",
            "Installing collected packages: karateclub\n",
            "Successfully installed karateclub-1.3.3\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting Levenshtein==0.27.3 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
            "Downloading levenshtein-0.27.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, gensim, python-Levenshtein\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "karateclub 1.3.3 requires pygsp, which is not installed.\n",
            "karateclub 1.3.3 requires networkx<2.7, but you have networkx 3.5 which is incompatible.\n",
            "karateclub 1.3.3 requires numpy<1.23.0, but you have numpy 1.26.4 which is incompatible.\n",
            "karateclub 1.3.3 requires pandas<=1.3.5, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Levenshtein-0.27.3 gensim-4.4.0 python-Levenshtein-0.27.3 rapidfuzz-3.14.3\n",
            "Collecting node2vec\n",
            "  Downloading node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from node2vec) (4.4.0)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from node2vec) (1.5.2)\n",
            "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from node2vec) (3.5)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from node2vec) (1.26.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from node2vec) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (2.0.1)\n",
            "Downloading node2vec-0.5.0-py3-none-any.whl (7.2 kB)\n",
            "Installing collected packages: node2vec\n",
            "Successfully installed node2vec-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygsp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpHJRmMEww_x",
        "outputId": "8b0bd129-73ba-46fe-dc15-d093a000426d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pygsp\n",
            "  Downloading pygsp-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pygsp) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pygsp) (1.16.3)\n",
            "Downloading pygsp-0.6.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygsp\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "karateclub 1.3.3 requires networkx<2.7, but you have networkx 3.5 which is incompatible.\n",
            "karateclub 1.3.3 requires numpy<1.23.0, but you have numpy 1.26.4 which is incompatible.\n",
            "karateclub 1.3.3 requires pandas<=1.3.5, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pygsp-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi47bKUP8PmO"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numba\n",
        "from numba import jit,njit, cuda\n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, f1_score, recall_score,precision_score, accuracy_score,average_precision_score,precision_recall_curve,auc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from karateclub import DeepWalk\n",
        "from node2vec import Node2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzqjDr3e_UPN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "converting to sparse matrix\n",
        "'''\n",
        "df=LEN=645 # or 645 if you use TWOSIDES dataset\n",
        "from scipy.sparse import coo_matrix\n",
        "nl=coo_matrix((df, df))\n",
        "nl.setdiag(1)\n",
        "#nl.toarray()\n",
        "values = nl.data\n",
        "indices = np.vstack((nl.row, nl.col))\n",
        "i = torch.LongTensor(indices)\n",
        "v = torch.FloatTensor(values)\n",
        "shape = nl.shape\n",
        "nl=torch.sparse_coo_tensor(i, v, torch.Size(shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydFHKdf_L_MW"
      },
      "outputs": [],
      "source": [
        "chemicalsub_drug = torch.load('/content/drive/MyDrive/Colab Notebooks/data/hyG_drug_645_kmer_3.pt')#reading the hypergraph for drugbank dataset for 3-mer. change it based on your choice\n",
        "data_dict = {\n",
        "        ('node', 'in', 'edge'): (chemicalsub_drug[:,0], chemicalsub_drug[:,1]),\n",
        "        ('edge', 'con', 'node'): (chemicalsub_drug[:,1], chemicalsub_drug[:,0])\n",
        "    }\n",
        "\n",
        "hyG = dgl.heterograph(data_dict)\n",
        "n_chemicalsub=822 #change the num of rows based on the dataset. row info. is available in data folder\n",
        "rows=n_chemicalsub\n",
        "n_hedge=LEN\n",
        "columns=n_hedge\n",
        "\n",
        "drug_X=nl\n",
        "hyG.ndata['h'] = {'edge' : torch.tensor(drug_X).type('torch.FloatTensor'), 'node' : torch.ones(rows, 128)}\n",
        "e_feat = torch.tensor(drug_X).type('torch.FloatTensor')\n",
        "v_feat = torch.ones(rows, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cModgoYc3WxK",
        "outputId": "b1a58db9-4179-4617-e179-2de3a5ef882c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Graph(num_nodes={'edge': 645, 'node': 822},\n",
              "      num_edges={('edge', 'con', 'node'): 31631, ('node', 'in', 'edge'): 31631},\n",
              "      metagraph=[('edge', 'node', 'con'), ('node', 'edge', 'in')])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "hyG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR3bvhsYjS6y"
      },
      "outputs": [],
      "source": [
        "#Data prep For Regular Graph\n",
        "src=[]\n",
        "dst=[]\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/data/edge_list_regular_graph_645.txt\") as fp:\n",
        "        for i,line in enumerate (fp):\n",
        "            info = line.strip().split()\n",
        "            src.append(info[0])\n",
        "            dst.append(info[1])\n",
        "\n",
        "src=np.asarray(src, dtype=np.int64) #to use in the data prerpocessing\n",
        "dst=np.asarray(dst, dtype=np.int64) #to use in the data prerpocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYhYK_AT82v0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd9452a-d40e-4f89-b4cd-b0d54b5e2860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph(num_nodes=645, num_edges=126946,\n",
            "      ndata_schemes={}\n",
            "      edata_schemes={})\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Regular Graph creation for the baselines\n",
        "\"\"\"\n",
        "from dgl.data import DGLDataset\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class KarateClubDataset(DGLDataset):\n",
        "    def __init__(self):\n",
        "        super().__init__(name='karate_club')\n",
        "    def process(self):\n",
        "        edges_src = torch.from_numpy(src)\n",
        "        edges_dst = torch.from_numpy(dst)\n",
        "        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=LEN)\n",
        "    def __getitem__(self, i):\n",
        "        return self.graph\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "dataset = KarateClubDataset()\n",
        "g = dataset[0]\n",
        "print(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOecEkgG_Dvq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "4da8f19c-c4a5-440b-8f42-70886846e9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "生成完毕，共有 12694 条边\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot compute fingerprint of empty list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-127962014.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mval_eids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mfunction1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0mval_eids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_eids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_eids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mtrain_eids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot compute fingerprint of empty list"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Regular Graph creation for the baselines\n",
        "*************another approach ***********************************\n",
        "'''\n",
        "np.random.seed(42)\n",
        "# Split edge set for training and testing\n",
        "u, v = g.edges()\n",
        "eids = np.arange(g.number_of_edges())\n",
        "eids = np.random.permutation(eids)\n",
        "eids=eids.tolist()\n",
        "test_size = int(len(eids) * 0.1)\n",
        "val_size = int(len(eids) * 0.1)\n",
        "train_size=len(eids)-(test_size+val_size)\n",
        "test_size1=int(test_size/2)\n",
        "\n",
        "\n",
        "T1=[]\n",
        "T2=[]\n",
        " numba.njit(target=\"cuda\")\n",
        " def func2(T1):\n",
        "  for i in eids:\n",
        "    m, n = int(u[i]), int(v[i])\n",
        "    c=(m,n)\n",
        "    T1.append(c)\n",
        "  return T1\n",
        "function1=jit(parallel=True) (func2)\n",
        "T1 = function1(T1)\n",
        "for i in eids:\n",
        "    m, n = int(u[i]), int(v[i])\n",
        "    T1.append((m, n))\n",
        "\n",
        "\n",
        " numba.njit(target=\"cuda\")\n",
        " def func3(T1,T2):\n",
        "  for i in T1:\n",
        "    m=i[0]\n",
        "    n=i[1]\n",
        "    c=(m,n)\n",
        "    d=(n,m)\n",
        "    if c not in T2 and d not in T2:\n",
        "      T2.append(c)\n",
        "    if len(T2)>=test_size1:\n",
        "      break\n",
        "  return T2\n",
        "  function1=jit(parallel=True) (func3)\n",
        " T2 = function1(T1,T2)\n",
        "\n",
        "seen_edges = set() # 用集合来加速 \"not in\" 判断\n",
        "\n",
        "for i in T1:\n",
        "    m, n = i[0], i[1]\n",
        "    c = (m, n)\n",
        "    d = (n, m)\n",
        "\n",
        "    # 如果正向和反向边都不在集合中\n",
        "    if c not in seen_edges and d not in seen_edges:\n",
        "        T2.append(c)\n",
        "        seen_edges.add(c) # 记录已存在的边\n",
        "\n",
        "    if len(T2) >= test_size1:\n",
        "        break\n",
        "\n",
        "\n",
        "test_eids=[]\n",
        "src_nodes = [i[0] for i in T2]\n",
        "dst_nodes = [i[1] for i in T2]\n",
        "\n",
        "# 2. 批量获取正向边 (m -> n) 的 ID\n",
        "# g.edge_ids 支持传入列表\n",
        "forward_eids = g.edge_ids(src_nodes, dst_nodes)\n",
        "\n",
        "# 3. 批量获取反向边 (n -> m) 的 ID\n",
        "backward_eids = g.edge_ids(dst_nodes, src_nodes)\n",
        "\n",
        "# 4. 合并结果并转回 Python 列表\n",
        "import torch # 确保导入了 torch\n",
        "test_eids = torch.cat([forward_eids, backward_eids]).tolist()\n",
        "\n",
        "print(f\"生成完毕，共有 {len(test_eids)} 条边\") #new adding\n",
        "'''\n",
        "numba.njit(target=\"cuda\")\n",
        "def func4(T2,test_eids,g):\n",
        "  for i in T2:\n",
        "    m= i[0]\n",
        "    n=i[1]\n",
        "    c=g.edge_id(m,n)\n",
        "    d=g.edge_id(n,m)\n",
        "    test_eids.append(c)\n",
        "    test_eids.append(d)\n",
        "  return test_eids\n",
        "function1=jit(parallel=True) (func4)\n",
        "test_eids = function1(T2,test_eids,g)\n",
        "'''\n",
        "\n",
        "val_eids=[]\n",
        "numba.njit(target=\"cuda\")\n",
        "def func5(eids,test_eids,val_eids):\n",
        "  i=0\n",
        "  for j in eids:\n",
        "    a=int(eids[i])\n",
        "    if a not in test_eids and len(val_eids)<val_size:\n",
        "      val_eids.append(a)\n",
        "    i=i+1\n",
        "  return val_eids\n",
        "function1=jit(parallel=True) (func5)\n",
        "val_eids = function1(eids,test_eids,val_eids)\n",
        "\n",
        "train_eids=[]\n",
        "numba.njit(target=\"cuda\")\n",
        "def func6(eids,test_eids,val_eids,train_eids):\n",
        "  i=0\n",
        "  for j in eids:\n",
        "    a=int(eids[i])\n",
        "    if a not in test_eids and a not in val_eids and len(train_eids)<train_size:\n",
        "      train_eids.append(a)\n",
        "    i=i+1\n",
        "  return train_eids\n",
        "function1=jit(parallel=True) (func6)\n",
        "train_eids = function1(eids,test_eids,val_eids,train_eids)\n",
        "\n",
        "# main one. splitting into train and test set.\n",
        "test_pos_u, test_pos_v = u[test_eids], v[test_eids]\n",
        "train_pos_u, train_pos_v = u[train_eids], v[train_eids]\n",
        "val_pos_u, val_pos_v = u[val_eids], v[val_eids]\n",
        "\n",
        "\n",
        "# Find all negative edges and split them for training and testing\n",
        "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
        "adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
        "neg_u, neg_v = np.where(adj_neg != 0)\n",
        "\n",
        "neg_eids = np.random.choice(len(neg_u), g.number_of_edges(),replace=False)\n",
        "g_=nx.from_numpy_matrix(adj_neg)\n",
        "g_=dgl.from_networkx(g_)\n",
        "\n",
        "T1=[]\n",
        "T2=[]\n",
        "numba.njit(target=\"cuda\")\n",
        "def func2(T1):\n",
        "  for i in neg_eids:\n",
        "    m, n = int(neg_u[i]), int(neg_v[i])\n",
        "    c=(m,n)\n",
        "    T1.append(c)\n",
        "  return T1\n",
        "function1=jit(parallel=True) (func2)\n",
        "T1 = function1(T1)\n",
        "\n",
        "\n",
        "numba.njit(target=\"cuda\")\n",
        "def func3(T1,T2):\n",
        "  for i in T1:\n",
        "    m=i[0]\n",
        "    n=i[1]\n",
        "    c=(m,n)\n",
        "    d=(n,m)\n",
        "    if c not in T2 and d not in T2:\n",
        "      T2.append(c)\n",
        "    if len(T2)>=test_size1:\n",
        "      break\n",
        "  return T2\n",
        "function1=jit(parallel=True) (func3)\n",
        "T2 = function1(T1,T2)\n",
        "\n",
        "\n",
        "test_neg_eids=[]\n",
        "numba.njit(target=\"cuda\")\n",
        "def func4(T2,test_eids,g):\n",
        "  for i in T2:\n",
        "    m= i[0]\n",
        "    n=i[1]\n",
        "    c=g.edge_id(m,n)\n",
        "    d=g.edge_id(n,m)\n",
        "    test_neg_eids.append(c)\n",
        "    test_neg_eids.append(d)\n",
        "  return test_neg_eids\n",
        "function1=jit(parallel=True) (func4)\n",
        "test_neg_eids = function1(T2,test_neg_eids,g_)\n",
        "\n",
        "val_neg_eids=[]\n",
        "numba.njit(target=\"cuda\")\n",
        "def func5(neg_eids,test_neg_eids,val_neg_eids):\n",
        "  i=0\n",
        "  for j in neg_eids:\n",
        "    a=int(neg_eids[i])\n",
        "    if a not in test_neg_eids and len(val_neg_eids)<(val_size):\n",
        "      val_neg_eids.append(a)\n",
        "    i=i+1\n",
        "  return val_neg_eids\n",
        "function1=jit(parallel=True) (func5)\n",
        "val_neg_eids = function1(neg_eids,test_neg_eids,val_neg_eids)\n",
        "\n",
        "train_neg_eids=[]\n",
        "numba.njit(target=\"cuda\")\n",
        "def func6(neg_eids,test_neg_eids,val_neg_eids,train_neg_eids):\n",
        "  i=0\n",
        "  for j in neg_eids:\n",
        "    a=int(neg_eids[i])\n",
        "    if a not in test_neg_eids and a not in val_neg_eids and len(train_neg_eids)<(train_size):\n",
        "      train_neg_eids.append(a)\n",
        "    i=i+1\n",
        "  return train_neg_eids\n",
        "function1=jit(parallel=True) (func6)\n",
        "train_neg_eids = function1(neg_eids,test_neg_eids,val_neg_eids,train_neg_eids)\n",
        "\n",
        "test_neg_u, test_neg_v = neg_u[test_neg_eids], neg_v[test_neg_eids]\n",
        "train_neg_u, train_neg_v = neg_u[train_neg_eids], neg_v[train_neg_eids]\n",
        "val_neg_u, val_neg_v = neg_u[val_neg_eids], neg_v[val_neg_eids]\n",
        "\n",
        "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())# this is just the edge_list\n",
        "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
        "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
        "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n",
        "val_pos_g = dgl.graph((val_pos_u, val_pos_v), num_nodes=g.number_of_nodes())\n",
        "val_neg_g = dgl.graph((val_neg_u, val_neg_v), num_nodes=g.number_of_nodes())\n",
        "\n",
        "print(\"Total number of edges: \",g_.number_of_edges())\n",
        "print(\"num of train POSITIVE edges: \",train_pos_g.number_of_edges(),\", num of test POSITIVE edges: \",test_pos_g.number_of_edges(),\"num of val POSITIVE edges: \",val_pos_g.number_of_edges())\n",
        "print(\"num of train NEGATIVE edges: \",train_neg_g.number_of_edges(),\", num of test NEGATIVE edges: \",test_neg_g.number_of_edges(),\"num of val NEGATIVE edges: \",val_neg_g.number_of_edges())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/TDC_DATA/TDC_DrugBank_useable.csv'\n",
        "\n",
        "print(f\"正在读取文件: {file_path}\")\n",
        "\n",
        "# 2. 读取 CSV 文件\n",
        "# header=None 假设文件没有列名，第一行就是数据。如果读取出来第一行是列名，请去掉 header=None\n",
        "df = pd.read_csv(file_path, header=None)\n",
        "\n",
        "# 3. 重新定义 df (原来的代码把它覆盖成数字了)\n",
        "# 假设第0列是药物ID (如 DB00001)，第1列是 SMILES 化学式\n",
        "df.columns = ['Drug_ID', 'SMILES']\n",
        "\n",
        "# 4. 重新构建字典 D\n",
        "# 根据你的报错代码逻辑：for j, k in D.items(): if i == k\n",
        "# i 是图中的节点索引 (0, 1, 2...)，所以 D 的 Value 应该是数字索引\n",
        "# D 的结构应该是：{ '药物ID': 数字索引 }\n",
        "D = {}\n",
        "for idx, row in df.iterrows():\n",
        "    # 这里的 idx 就是 0, 1, 2... 对应图中的节点编号\n",
        "    drug_id = row['Drug_ID']\n",
        "    D[drug_id] = idx\n",
        "\n",
        "# 5. 重新构建字典 D_1 (用于把 ID 映射回 SMILES)\n",
        "D_1 = {}\n",
        "for idx, row in df.iterrows():\n",
        "    D_1[row['Drug_ID']] = row['SMILES']\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"成功恢复数据！\")\n",
        "print(f\"药物总数: {len(df)}\")\n",
        "print(f\"字典 D 大小: {len(D)}\")\n",
        "print(f\"字典 D_1 大小: {len(D_1)}\")\n",
        "print(\"-\" * 30)\n",
        "print(\"前5行数据预览：\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86CIasz8-ylB",
        "outputId": "f42e166f-2540-41b1-8ec5-343d32a31a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在读取文件: /content/drive/MyDrive/Colab Notebooks/TDC_DATA/TDC_DrugBank_useable.csv\n",
            "------------------------------\n",
            "成功恢复数据！\n",
            "药物总数: 1706\n",
            "字典 D 大小: 1706\n",
            "字典 D_1 大小: 1706\n",
            "------------------------------\n",
            "前5行数据预览：\n",
            "   Drug_ID                                             SMILES\n",
            "0  DB04571                CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1\n",
            "1  DB00460  COC(=O)CCC1=C2NC(\\C=C3/N=C(/C=C4\\N\\C(=C/C5=N/C...\n",
            "2  DB00855                                    NCC(=O)CCC(O)=O\n",
            "3  DB09536                                           O=[Ti]=O\n",
            "4  DB01600              CC(C(O)=O)C1=CC=C(S1)C(=O)C1=CC=CC=C1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Data prep for CASTER. This part returns negative edges\n",
        "'''\n",
        "import pandas as pd\n",
        "\n",
        "# 1. 准备工作：反转字典 D\n",
        "# 原代码逻辑是：如果 i == k (value)，取 j (key)。\n",
        "# 这说明你需要通过 Value 查 Key。为了速度，我们需要把 D 反转过来。\n",
        "# 假设 D 原本是 {'DB001': 0, 'DB002': 1}，我们需要变成 {0: 'DB001', 1: 'DB002'}\n",
        "D_reverse = {v: k for k, v in D.items()}\n",
        "\n",
        "# 2. 确保 D_1 格式正确 (直接用 ID 查 SMILES)\n",
        "# D_1 保持原样即可: {'DB001': 'C..C', ...}\n",
        "\n",
        "# 3. 创建 DataFrame 并进行极速映射\n",
        "df_re_reg = pd.DataFrame()\n",
        "\n",
        "# --- 处理 Drug 1 (neg_u) ---\n",
        "# 将 neg_u 转为 Series 以便使用 map\n",
        "s_neg_u = pd.Series(neg_u)\n",
        "\n",
        "# 第一步：映射 ID (对应你的 neg_u_s)\n",
        "# map 会瞬间根据 D_reverse 将索引换成 ID\n",
        "df_re_reg['Drug1_ID'] = s_neg_u.map(D_reverse)\n",
        "\n",
        "# 第二步：映射 SMILES (对应你的 neg_u_s1)\n",
        "# 直接用刚刚生成的 ID 列去映射 D_1\n",
        "df_re_reg['Drug1_SMILES'] = df_re_reg['Drug1_ID'].map(D_1)\n",
        "\n",
        "\n",
        "# --- 处理 Drug 2 (neg_v) ---\n",
        "s_neg_v = pd.Series(neg_v)\n",
        "\n",
        "# 第一步：映射 ID\n",
        "df_re_reg['Drug2_ID'] = s_neg_v.map(D_reverse)\n",
        "\n",
        "# 第二步：映射 SMILES\n",
        "df_re_reg['Drug2_SMILES'] = df_re_reg['Drug2_ID'].map(D_1)\n",
        "\n",
        "\n",
        "# --- 添加标签 ---\n",
        "# 负样本标签通常为 0\n",
        "df_re_reg['label'] = 0\n",
        "\n",
        "# --- 最终检查 ---\n",
        "print(\"处理完成！前5行预览：\")\n",
        "print(df_re_reg.head())\n",
        "\n",
        "# 检查是否有匹配失败的情况 (NaN)\n",
        "if df_re_reg.isnull().values.any():\n",
        "    print(\"警告: 存在匹配失败的数据 (NaN)，请检查字典是否覆盖了所有索引。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm2-BMANpoo0",
        "outputId": "0cebbe0c-1530-4244-ced7-b889c5c1911a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "处理完成！前5行预览：\n",
            "  Drug1_ID                         Drug1_SMILES Drug2_ID  \\\n",
            "0  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB00855   \n",
            "1  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB09536   \n",
            "2  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB01600   \n",
            "3  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB09000   \n",
            "4  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB00553   \n",
            "\n",
            "                                 Drug2_SMILES  label  \n",
            "0                             NCC(=O)CCC(O)=O      0  \n",
            "1                                    O=[Ti]=O      0  \n",
            "2       CC(C(O)=O)C1=CC=C(S1)C(=O)C1=CC=CC=C1      0  \n",
            "3  CC(CN(C)C)CN1C2=CC=CC=C2SC2=C1C=C(C=C2)C#N      0  \n",
            "4              COC1=C2OC(=O)C=CC2=CC2=C1OC=C2      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. 准备工作：确保字典逻辑正确\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# 假设 D 是 { 'Drug_ID': 数字索引 }\n",
        "# 你的代码逻辑是用 数字索引 查 Drug_ID，所以我们需要反转 D\n",
        "# D_reverse: { 数字索引: 'Drug_ID' }\n",
        "D_reverse = {v: k for k, v in D.items()}\n",
        "\n",
        "# D_1 保持不变: { 'Drug_ID': 'SMILES' }\n",
        "# 确保 D_1 已经构建好（如果还没构建，请保留你之前的 D_1 构建代码）\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. 核心映射 (替代 Numba 循环)\n",
        "# ---------------------------------------------------------\n",
        "print(\"开始处理正样本...\")\n",
        "\n",
        "# 创建临时 DataFrame，放入原始数据 u 和 v\n",
        "df_pos_temp = pd.DataFrame({'u_origin': u, 'v_origin': v})\n",
        "\n",
        "# 第一步：将 索引(u) 映射回 ID (u_s)\n",
        "# 使用 map(D_reverse) 替代你的第一个 Numba 函数\n",
        "df_pos_temp['Drug1_ID'] = df_pos_temp['u_origin'].map(D_reverse)\n",
        "df_pos_temp['Drug2_ID'] = df_pos_temp['v_origin'].map(D_reverse)\n",
        "\n",
        "# 第二步：将 ID (u_s) 映射到 SMILES (u_s1)\n",
        "# 使用 map(D_1) 替代你的第二个 Numba 函数\n",
        "# 加上 .astype(str) 确保它是字符串再去查表，防止类型不匹配\n",
        "df_pos_temp['Drug1_SMILES'] = df_pos_temp['Drug1_ID'].astype(str).map(D_1)\n",
        "df_pos_temp['Drug2_SMILES'] = df_pos_temp['Drug2_ID'].astype(str).map(D_1)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. 添加标签并生成最终结果\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# 正样本：标签全部设为 1\n",
        "LEN = len(df_pos_temp)\n",
        "a_list_pos = [1] * LEN\n",
        "df_pos_temp['label'] = a_list_pos\n",
        "\n",
        "# 整理列的顺序，使其与负样本 DataFrame 结构完全一致\n",
        "df_pos_final = df_pos_temp[['Drug1_ID', 'Drug1_SMILES', 'Drug2_ID', 'Drug2_SMILES', 'label']]\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. 检查结果\n",
        "# ---------------------------------------------------------\n",
        "print(\"正样本处理完成！\")\n",
        "print(f\"数据行数: {len(df_pos_final)}\")\n",
        "print(df_pos_final.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4QdtUluT8gp",
        "outputId": "649b1fb4-448b-45b6-9b5e-08f048a508fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始处理正样本...\n",
            "正样本处理完成！\n",
            "数据行数: 126946\n",
            "  Drug1_ID                         Drug1_SMILES Drug2_ID  \\\n",
            "0  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB00460   \n",
            "1  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB11630   \n",
            "2  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB01168   \n",
            "3  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB01409   \n",
            "4  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB00387   \n",
            "\n",
            "                                        Drug2_SMILES  label  \n",
            "0  COC(=O)CCC1=C2NC(\\C=C3/N=C(/C=C4\\N\\C(=C/C5=N/C...      1  \n",
            "1  OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)...      1  \n",
            "2                       CNNCC1=CC=C(C=C1)C(=O)NC(C)C      1  \n",
            "3  [H][C@]12O[C@@]1([H])[C@]1([H])C[C@@]([H])(C[C...      1  \n",
            "4                 OC(CCN1CCCC1)(C1CCCCC1)C1=CC=CC=C1      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. 准备字典 (反向查 ID)\n",
        "# 假设 D 是 {ID: 索引}，我们需要 {索引: ID}\n",
        "D_reverse = {v: k for k, v in D.items()}\n",
        "\n",
        "print(\"正在构建负样本 DataFrame...\")\n",
        "\n",
        "# 2. 创建 DataFrame\n",
        "df_re_reg = pd.DataFrame()\n",
        "\n",
        "# 确保 neg_u 是列表或数组\n",
        "df_re_reg['idx_u'] = neg_u\n",
        "df_re_reg['idx_v'] = neg_v\n",
        "\n",
        "# 3. 极速映射 (Map)\n",
        "# 索引 -> ID\n",
        "df_re_reg['Drug1_ID'] = df_re_reg['idx_u'].map(D_reverse)\n",
        "df_re_reg['Drug2_ID'] = df_re_reg['idx_v'].map(D_reverse)\n",
        "\n",
        "# ID -> SMILES\n",
        "# 确保转为字符串去查\n",
        "df_re_reg['Drug1_SMILES'] = df_re_reg['Drug1_ID'].astype(str).map(D_1)\n",
        "df_re_reg['Drug2_SMILES'] = df_re_reg['Drug2_ID'].astype(str).map(D_1)\n",
        "\n",
        "# 4. 添加标签 (负样本为 0)\n",
        "df_re_reg['label'] = 0\n",
        "\n",
        "# 5. 清洗列 (只保留需要的)\n",
        "df_re_reg = df_re_reg[['Drug1_ID', 'Drug1_SMILES', 'Drug2_ID', 'Drug2_SMILES', 'label']]\n",
        "\n",
        "# 6. 检查结果\n",
        "print(\"-\" * 30)\n",
        "print(f\"✅ 负样本处理完成！数量: {len(df_re_reg)}\")\n",
        "print(df_re_reg.head())\n",
        "\n",
        "# 再次检查是否为空\n",
        "if len(df_re_reg) == 0:\n",
        "    print(\"❌ 警告：依然是 0，请检查 neg_u 是否为空！\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA1zc3XBsFiG",
        "outputId": "b3e147e3-c343-4ec2-bd22-656bd1a3698d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在构建负样本 DataFrame...\n",
            "------------------------------\n",
            "✅ 负样本处理完成！数量: 288434\n",
            "  Drug1_ID                         Drug1_SMILES Drug2_ID  \\\n",
            "0  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB00855   \n",
            "1  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB09536   \n",
            "2  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB01600   \n",
            "3  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB09000   \n",
            "4  DB04571  CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1  DB00553   \n",
            "\n",
            "                                 Drug2_SMILES  label  \n",
            "0                             NCC(=O)CCC(O)=O      0  \n",
            "1                                    O=[Ti]=O      0  \n",
            "2       CC(C(O)=O)C1=CC=C(S1)C(=O)C1=CC=CC=C1      0  \n",
            "3  CC(CN(C)C)CN1C2=CC=CC=C2SC2=C1C=C(C=C2)C#N      0  \n",
            "4              COC1=C2OC(=O)C=CC2=CC2=C1OC=C2      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. 获取正样本 (直接使用上一段代码生成的 df_pos_final)\n",
        "# 注意：如果你上一段代码生成的变量名是 df_pos_temp，请这里改一下名字\n",
        "df_re = df_pos_final\n",
        "\n",
        "# 打印一下看看长度对不对\n",
        "print(f\"正样本数量: {len(df_re)}\")\n",
        "print(f\"负样本数量: {len(df_re_reg)}\")\n",
        "\n",
        "# 2. 合并正样本 (df_re) 和 负样本 (df_re_reg)\n",
        "# 注意：Pandas 新版本推荐用 pd.concat 而不是 append\n",
        "df_total = pd.concat([df_re, df_re_reg], ignore_index=True)\n",
        "\n",
        "# 3. 打乱数据顺序 (Shuffle)\n",
        "df_total = df_total.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# 4. 保存文件\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/TDC_DATA/TDC_DrugBank_FOR_CASTER.csv'\n",
        "df_total.to_csv(save_path, index=None)\n",
        "\n",
        "print(f\"文件已保存至: {save_path}\")\n",
        "print(df_total.head()) # 预览结果"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnT61Sp-UqCB",
        "outputId": "7c75e353-abab-49f3-dbe0-2a6bbf99c347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正样本数量: 126946\n",
            "负样本数量: 288434\n",
            "文件已保存至: /content/drive/MyDrive/Colab Notebooks/TDC_DATA/TDC_DrugBank_FOR_CASTER.csv\n",
            "  Drug1_ID                                       Drug1_SMILES Drug2_ID  \\\n",
            "0  DB06770                                      OCC1=CC=CC=C1  DB00394   \n",
            "1  DB00289               CNCC[C@@H](OC1=CC=CC=C1C)C1=CC=CC=C1  DB01628   \n",
            "2  DB13595  O.[Mg++].[Al+3].[Al+3].[O-][Si]([O-])([O-])[O-...  DB00559   \n",
            "3  DB00401  COC(=O)C1=C(C)NC(C)=C(C1C1=CC=CC=C1[N+]([O-])=...  DB01159   \n",
            "4  DB01340  CCOC(=O)[C@H](CCC1=CC=CC=C1)N[C@H]1CCCN2CCC[C@...  DB01002   \n",
            "\n",
            "                                        Drug2_SMILES  label  \n",
            "0  [H][C@@]12C[C@H](C)[C@](OC(=O)CC)(C(=O)COC(=O)...      0  \n",
            "1  CC1=NC=C(C=C1)C1=C(C=C(Cl)C=N1)C1=CC=C(C=C1)S(...      0  \n",
            "2  COC1=CC=CC=C1OC1=C(NS(=O)(=O)C2=CC=C(C=C2)C(C)...      1  \n",
            "3                               [H]C(Cl)(Br)C(F)(F)F      1  \n",
            "4              CCCCN1CCCC[C@H]1C(=O)NC1=C(C)C=CC=C1C      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NQ16ZlGNH47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d5acdb-a968-495c-a33a-9a591311d056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of edges:  126946\n",
            "num of train POSITIVE edges:  101558 , num of test POSITIVE edges:  12694 num of val POSITIVE edges:  12694\n",
            "num of train NEGATIVE edges:  101558 , num of test NEGATIVE edges:  12694 num of val NEGATIVE edges:  12694\n"
          ]
        }
      ],
      "source": [
        "print(\"Total number of edges: \",g.number_of_edges())\n",
        "print(\"num of train POSITIVE edges: \",train_pos_g.number_of_edges(),\", num of test POSITIVE edges: \",test_pos_g.number_of_edges(),\"num of val POSITIVE edges: \",val_pos_g.number_of_edges())\n",
        "print(\"num of train NEGATIVE edges: \",train_neg_g.number_of_edges(),\", num of test NEGATIVE edges: \",test_neg_g.number_of_edges(),\"num of val NEGATIVE edges: \",val_neg_g.number_of_edges())\n",
        "\n",
        "g_for_baseline = dgl.remove_edges(g, test_eids)\n",
        "#dgl.save_graphs(\"drive/My Drive/Colab Notebooks/Hypergraph_my/a.bin\",g_for_baseline)\n",
        "g_for_baseline = dgl.add_self_loop(g_for_baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_U_zGixKUbG"
      },
      "outputs": [],
      "source": [
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "def compute_auc(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
        "    precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
        "    auc_precision_recall = auc(recall, precision)\n",
        "    return roc_auc_score(labels, scores),auc(recall, precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBFa3jJD9Jo2"
      },
      "outputs": [],
      "source": [
        "import dgl.function as fn\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h # here h is 822 drug features and g is the pos/neg train/test graph which is nothing but edge lis\n",
        "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
        "            return g.edata['score'][:, 0]\n",
        "\n",
        "class MLPPredictor(nn.Module):\n",
        "    def __init__(self, h_feats):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
        "        self.W2 = nn.Linear(h_feats, 1)\n",
        "\n",
        "    def apply_edges(self, edges):\n",
        "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
        "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            g.apply_edges(self.apply_edges)\n",
        "            return g.edata['score']\n",
        "\n",
        "\n",
        "decoder = MLPPredictor(128)\n",
        "#decoder = DotPredictor()# You can replace DotPredictor with MLPPredictor.\n",
        "#opt = torch.optim.Adam(list(model.parameters()) + list(pred.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvzS_z3OGNdd"
      },
      "outputs": [],
      "source": [
        "class HyGNN(nn.Module):\n",
        "    def __init__(self, input_dim, query_dim, vertex_dim, edge_dim, dropout):\n",
        "        super(HyGNN, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.query_dim = query_dim\n",
        "        self.in_first_layer = torch.nn.Linear(input_dim, vertex_dim)\n",
        "        self.not_in_first_layer = torch.nn.Linear(vertex_dim, vertex_dim)\n",
        "        self.w6 = torch.nn.Linear(edge_dim, query_dim)\n",
        "        self.w5 = torch.nn.Linear(vertex_dim, query_dim)\n",
        "        self.w4 = torch.nn.Linear(vertex_dim, edge_dim)\n",
        "        self.w3 = torch.nn.Linear(vertex_dim, query_dim)\n",
        "        self.w2 = torch.nn.Linear(edge_dim, query_dim)\n",
        "        self.w1 = torch.nn.Linear(edge_dim, vertex_dim)\n",
        "\n",
        "\n",
        "    def red_function(self, nodes):\n",
        "        attention_score = F.softmax((nodes.mailbox['Attn']), dim=1)\n",
        "        aggregated = torch.sum(attention_score.unsqueeze(-1) * nodes.mailbox['v'], dim=1)\n",
        "        return {'h': aggregated}\n",
        "\n",
        "\n",
        "    def attention(self, edges):\n",
        "        attn_score = F.leaky_relu((edges.src['k'] * edges.dst['q']).sum(-1))\n",
        "        return {'Attn': attn_score/np.sqrt(self.query_dim)}\n",
        "\n",
        "    def msg_fucntion(self, edges):\n",
        "        return {'v': edges.src['v'], 'Attn': edges.data['Attn']}\n",
        "\n",
        "\n",
        "    def forward(self, hyG, vfeat, efeat, first_layer, last_layer):\n",
        "            if first_layer:\n",
        "                feat_e = self.in_first_layer(efeat)\n",
        "            else:\n",
        "                feat_e = self.not_in_first_layer(efeat)\n",
        "            feat_v = vfeat\n",
        "            #Hyperedge-level attention\n",
        "            hyG.ndata['h'] = {'edge': feat_e}\n",
        "            hyG.ndata['k'] = {'edge' : self.w2(feat_e)}\n",
        "            hyG.ndata['v'] = {'edge' : self.w1(feat_e)}\n",
        "            hyG.ndata['q'] = {'node' : self.w3(feat_v)}\n",
        "            hyG.apply_edges(self.attention, etype='con')\n",
        "            hyG.update_all(self.msg_fucntion, self.red_function, etype='con')\n",
        "\n",
        "            #Node-level attention\n",
        "            feat_v = hyG.ndata['h']['node']\n",
        "            hyG.ndata['k'] = {'node' : self.w5(feat_v)}\n",
        "            hyG.ndata['v'] = {'node' : self.w4(feat_v)}\n",
        "            hyG.ndata['q'] = {'edge' : self.w6(feat_e)}\n",
        "            hyG.apply_edges(self.attention, etype='in')\n",
        "            hyG.update_all(self.msg_fucntion, self.red_function, etype='in')\n",
        "            feat_e = hyG.ndata['h']['edge']\n",
        "\n",
        "            if not last_layer :\n",
        "                feat_v = F.dropout(feat_v, self.dropout)\n",
        "            if last_layer:\n",
        "\n",
        "                return feat_v, feat_e\n",
        "            else:\n",
        "                return [hyG, feat_v, feat_e]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17tBcT8M-BGG"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.gat1 = HyGNN(drug_X.shape[1],64,128,128,0.5)\n",
        "\n",
        "    def forward(self,hyG, v_feat, e_feat,f,l):\n",
        "        h = self.gat1(hyG, v_feat, e_feat,f,l)\n",
        "        return h\n",
        "model = Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jLSMx5T-ItX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "outputId": "0d896cae-ee63-497d-f1f6-389fef5a8dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, train loss: 0.6932, val loss: 0.6960 (best val loss: 0.6960)\n",
            "In epoch 10, train loss: 0.6907, val loss: 0.6905 (best val loss: 0.6905)\n",
            "In epoch 20, train loss: 0.6494, val loss: 0.6541 (best val loss: 0.6541)\n",
            "In epoch 30, train loss: 0.5489, val loss: 0.5983 (best val loss: 0.5983)\n",
            "In epoch 40, train loss: 0.4667, val loss: 0.4800 (best val loss: 0.4800)\n",
            "In epoch 50, train loss: 0.4052, val loss: 0.4078 (best val loss: 0.4078)\n",
            "In epoch 60, train loss: 0.3799, val loss: 0.3790 (best val loss: 0.3790)\n",
            "In epoch 70, train loss: 0.3614, val loss: 0.3505 (best val loss: 0.3505)\n",
            "In epoch 80, train loss: 0.3514, val loss: 0.3401 (best val loss: 0.3401)\n",
            "In epoch 90, train loss: 0.3412, val loss: 0.3271 (best val loss: 0.3271)\n",
            "In epoch 100, train loss: 0.3332, val loss: 0.3177 (best val loss: 0.3177)\n",
            "In epoch 110, train loss: 0.3280, val loss: 0.3031 (best val loss: 0.3031)\n",
            "In epoch 120, train loss: 0.3238, val loss: 0.3007 (best val loss: 0.3007)\n",
            "In epoch 130, train loss: 0.3216, val loss: 0.2968 (best val loss: 0.2965)\n",
            "In epoch 140, train loss: 0.3193, val loss: 0.2917 (best val loss: 0.2917)\n",
            "In epoch 150, train loss: 0.3165, val loss: 0.2915 (best val loss: 0.2912)\n",
            "In epoch 160, train loss: 0.3308, val loss: 0.3057 (best val loss: 0.2912)\n",
            "In epoch 170, train loss: 0.3193, val loss: 0.2930 (best val loss: 0.2912)\n",
            "In epoch 180, train loss: 0.3144, val loss: 0.2870 (best val loss: 0.2870)\n",
            "In epoch 190, train loss: 0.3113, val loss: 0.2853 (best val loss: 0.2846)\n",
            "In epoch 200, train loss: 0.3090, val loss: 0.2867 (best val loss: 0.2841)\n",
            "In epoch 210, train loss: 0.3078, val loss: 0.2865 (best val loss: 0.2841)\n",
            "In epoch 220, train loss: 0.3072, val loss: 0.2875 (best val loss: 0.2841)\n",
            "In epoch 230, train loss: 0.3067, val loss: 0.2870 (best val loss: 0.2841)\n",
            "In epoch 240, train loss: 0.3062, val loss: 0.2873 (best val loss: 0.2841)\n",
            "In epoch 250, train loss: 0.3059, val loss: 0.2877 (best val loss: 0.2841)\n",
            "In epoch 260, train loss: 0.3056, val loss: 0.2894 (best val loss: 0.2841)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1622261355.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ----------- 3. set up loss and optimizer -------------- #\n",
        "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), decoder.parameters()), lr=0.005)\n",
        "best_val_loss=1e10\n",
        "patience=0\n",
        "\n",
        "for e in range(500):\n",
        "    # forward\n",
        "    model.train()\n",
        "    h=model(hyG, v_feat, e_feat,True,True)\n",
        "    h=h[1]\n",
        "    pos_score = decoder(train_pos_g, h)\n",
        "    neg_score = decoder(train_neg_g, h)\n",
        "    loss = compute_loss(pos_score, neg_score)\n",
        "\n",
        "    # backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      pos_score = decoder(val_pos_g, h)\n",
        "      neg_score = decoder(val_neg_g, h)\n",
        "      #val_acc=compute_auc(pos_score, neg_score)\n",
        "      val_loss = compute_loss(pos_score, neg_score)\n",
        "      if val_loss<best_val_loss:\n",
        "        best_val_loss=val_loss\n",
        "        H=h\n",
        "        E=e\n",
        "        patience=0\n",
        "        torch.save(decoder.state_dict(),'latest.pth')\n",
        "      else:\n",
        "        patience+=1\n",
        "    if patience>200:\n",
        "      break\n",
        "\n",
        "    if e % 10 == 0:\n",
        "      print('In epoch {}, train loss: {:.4f}, val loss: {:.4f} (best val loss: {:.4f})'.format(e, loss, val_loss, best_val_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcRFtgZoPWSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292d44d5-03ca-4551-8598-dc4dbd662610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Best Epoch: 191, Accuracy: 0.8577, Precision: 0.8744, Recall: 0.8502, F1-score 0.8621, ROC-AUC 0.9403, PR-AUC 0.9468\n"
          ]
        }
      ],
      "source": [
        "decoder.load_state_dict(torch.load('latest.pth'))\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "\n",
        "  pos_score = decoder(test_pos_g, H)\n",
        "  neg_score = decoder(test_neg_g, H)\n",
        "  test_acc=compute_auc(pos_score, neg_score)\n",
        "\n",
        "scores = torch.cat([pos_score, neg_score])\n",
        "labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "\n",
        "m1 = tf.keras.metrics.BinaryAccuracy()\n",
        "m1.update_state(labels,scores)\n",
        "\n",
        "sig_scores=F.sigmoid(scores)\n",
        "m2 = tf.keras.metrics.Precision()\n",
        "m2.update_state(labels,sig_scores)\n",
        "M2=m2.result().numpy()\n",
        "\n",
        "m3 = tf.keras.metrics.Recall()\n",
        "m3.update_state(labels,sig_scores)\n",
        "M3=m3.result().numpy()\n",
        "\n",
        "F1=2*(M2*M3)/(M2+M3)\n",
        "print(' Best Epoch: {}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score {:.4f}, ROC-AUC {:.4f}, PR-AUC {:.4f}'.format( E,m1.result().numpy(), M2, M3, F1,test_acc[0],test_acc[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oad6nWOtTaUj"
      },
      "outputs": [],
      "source": [
        "################################################## Case study #########################################3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6anF0igYM2kW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "5b502777-5247-49df-c439-5ec6a86c69fa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 25388 is out of bounds for dimension 0 with size 25388",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2457334233.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m25388\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m26410\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# range s ur choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0msig_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0.0000001\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# use a large threshold say 0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(sig_scores[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 25388 is out of bounds for dimension 0 with size 25388"
          ]
        }
      ],
      "source": [
        "lst=[]\n",
        "for i in range (25388,26410): # range s ur choice\n",
        "  if sig_scores[i]<=0.0000001: # use a large threshold say 0.9\n",
        "    #print(sig_scores[i])\n",
        "    lst.append(i)\n",
        "print(len(lst))\n",
        "sig_scores[lst[0]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 将 tensor 或 list 转为 pandas Series，方便统计\n",
        "# 如果 sig_scores 是 tensor，先转 numpy: sig_scores.cpu().numpy()\n",
        "scores = pd.Series(sig_scores)\n",
        "\n",
        "print(\"=== 分数统计情况 ===\")\n",
        "print(f\"最大值 (最像正样本): {scores.max()}\")\n",
        "print(f\"最小值 (最像负样本): {scores.min()}\")\n",
        "print(f\"平均值: {scores.mean()}\")\n",
        "print(f\"中位数: {scores.median()}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"前 10 个分数预览：\")\n",
        "print(scores.head(10).tolist())\n",
        "\n",
        "# 画个图直观看看分布\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(scores, bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('Prediction Scores Distribution')\n",
        "plt.xlabel('Score (Probability)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "7HOWCq2s1vXW",
        "outputId": "cd577374-d64b-477a-c17f-23821bdacaa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 分数统计情况 ===\n",
            "最大值 (最像正样本): 0.9999998807907104\n",
            "最小值 (最像负样本): 8.399445050599752e-07\n",
            "平均值: 0.5013910531997681\n",
            "中位数: 0.4663342833518982\n",
            "------------------------------\n",
            "前 10 个分数预览：\n",
            "[0.997776448726654, 0.9913724660873413, 0.20465360581874847, 0.3375302255153656, 0.16254056990146637, 0.998307466506958, 0.6039577722549438, 0.9998317956924438, 0.9742211103439331, 0.8979911804199219]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lst = []\n",
        "\n",
        "for i in range(len(sig_scores)):\n",
        "    if sig_scores[i] <= 0.01: # 阈值不确定\n",
        "        lst.append(i)\n",
        "\n",
        "print(f\"满足条件的样本数量: {len(lst)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IojiF851VPo",
        "outputId": "957877d8-0dfa-4afe-8432-6d23995360c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "满足条件的样本数量: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMzBtsy3cThR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb7c121-3b27-4b28-a010-3ef0af823a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0037) tensor(304) tensor(213)\n",
            "tensor(0.0100) tensor(329) tensor(117)\n",
            "tensor(0.0039) tensor(213) tensor(304)\n",
            "tensor(0.0046) tensor(642) tensor(165)\n",
            "tensor(0.0088) tensor(359) tensor(632)\n",
            "tensor(0.0003) tensor(623) tensor(631)\n",
            "tensor(0.0006) tensor(631) tensor(510)\n",
            "tensor(0.0094) tensor(420) tensor(601)\n",
            "tensor(0.0060) tensor(260) tensor(642)\n",
            "tensor(0.0048) tensor(520) tensor(625)\n",
            "tensor(0.0095) tensor(513) tensor(620)\n",
            "tensor(0.0086) tensor(481) tensor(633)\n",
            "tensor(0.0048) tensor(628) tensor(576)\n",
            "tensor(0.0028) tensor(643) tensor(415)\n",
            "tensor(0.0024) tensor(603) tensor(622)\n",
            "tensor(0.0021) tensor(628) tensor(639)\n",
            "tensor(0.0053) tensor(641) tensor(580)\n",
            "tensor(0.0093) tensor(505) tensor(308)\n",
            "tensor(0.0038) tensor(410) tensor(630)\n",
            "tensor(0.0044) tensor(563) tensor(462)\n",
            "tensor(6.5682e-06) tensor(246) tensor(627)\n",
            "tensor(0.0069) tensor(443) tensor(5)\n",
            "tensor(2.8555e-05) tensor(627) tensor(169)\n",
            "tensor(0.0081) tensor(610) tensor(632)\n",
            "tensor(0.0081) tensor(540) tensor(476)\n",
            "tensor(0.0076) tensor(468) tensor(570)\n",
            "tensor(0.0065) tensor(505) tensor(306)\n",
            "tensor(0.0085) tensor(591) tensor(509)\n",
            "tensor(0.0031) tensor(232) tensor(636)\n",
            "tensor(0.0082) tensor(348) tensor(466)\n",
            "tensor(0.0095) tensor(35) tensor(641)\n",
            "tensor(0.0019) tensor(642) tensor(310)\n",
            "tensor(0.0040) tensor(563) tensor(299)\n",
            "tensor(0.0031) tensor(527) tensor(641)\n",
            "tensor(0.0096) tensor(396) tensor(426)\n",
            "tensor(0.0004) tensor(614) tensor(642)\n",
            "tensor(0.0070) tensor(638) tensor(5)\n",
            "tensor(0.0086) tensor(636) tensor(261)\n",
            "tensor(0.0004) tensor(306) tensor(643)\n",
            "tensor(0.0037) tensor(493) tensor(639)\n",
            "tensor(0.0077) tensor(634) tensor(234)\n",
            "tensor(0.0076) tensor(226) tensor(640)\n",
            "tensor(4.5029e-05) tensor(627) tensor(580)\n",
            "tensor(0.0060) tensor(551) tensor(633)\n",
            "tensor(0.0053) tensor(491) tensor(618)\n",
            "tensor(0.0087) tensor(541) tensor(524)\n",
            "tensor(0.0020) tensor(443) tensor(601)\n",
            "tensor(0.0053) tensor(644) tensor(226)\n",
            "tensor(0.0076) tensor(587) tensor(466)\n",
            "tensor(0.0079) tensor(588) tensor(574)\n",
            "tensor(0.0028) tensor(570) tensor(587)\n",
            "tensor(0.0011) tensor(642) tensor(456)\n",
            "tensor(0.0077) tensor(127) tensor(635)\n",
            "tensor(0.0095) tensor(447) tensor(370)\n",
            "tensor(0.0030) tensor(644) tensor(545)\n",
            "tensor(0.0043) tensor(348) tensor(589)\n",
            "tensor(0.0020) tensor(308) tensor(635)\n",
            "tensor(0.0058) tensor(426) tensor(505)\n",
            "tensor(0.0021) tensor(284) tensor(624)\n",
            "tensor(0.0002) tensor(622) tensor(642)\n",
            "tensor(0.0048) tensor(624) tensor(299)\n",
            "tensor(0.0047) tensor(634) tensor(540)\n",
            "tensor(0.0059) tensor(635) tensor(439)\n",
            "tensor(0.0044) tensor(102) tensor(636)\n",
            "tensor(0.0089) tensor(187) tensor(601)\n",
            "tensor(0.0015) tensor(636) tensor(606)\n",
            "tensor(0.0025) tensor(49) tensor(642)\n",
            "tensor(0.0020) tensor(577) tensor(284)\n",
            "tensor(0.0015) tensor(641) tensor(414)\n",
            "tensor(0.0099) tensor(628) tensor(569)\n",
            "tensor(0.0026) tensor(632) tensor(639)\n",
            "tensor(4.0932e-05) tensor(627) tensor(382)\n",
            "tensor(0.0026) tensor(584) tensor(640)\n",
            "tensor(0.0074) tensor(624) tensor(637)\n",
            "tensor(0.0071) tensor(410) tensor(624)\n",
            "tensor(0.0010) tensor(172) tensor(631)\n",
            "tensor(0.0059) tensor(481) tensor(635)\n",
            "tensor(0.0042) tensor(266) tensor(601)\n",
            "tensor(0.0082) tensor(329) tensor(587)\n",
            "tensor(0.0084) tensor(583) tensor(609)\n",
            "tensor(0.0004) tensor(468) tensor(631)\n",
            "tensor(0.0036) tensor(83) tensor(644)\n",
            "tensor(0.0054) tensor(639) tensor(524)\n",
            "tensor(0.0009) tensor(631) tensor(612)\n",
            "tensor(0.0062) tensor(262) tensor(641)\n",
            "tensor(0.0028) tensor(641) tensor(119)\n",
            "tensor(0.0040) tensor(632) tensor(324)\n",
            "tensor(0.0041) tensor(644) tensor(391)\n",
            "tensor(0.0094) tensor(613) tensor(548)\n",
            "tensor(0.0050) tensor(299) tensor(549)\n",
            "tensor(0.0059) tensor(601) tensor(183)\n",
            "tensor(0.0010) tensor(447) tensor(630)\n",
            "tensor(0.0089) tensor(568) tensor(635)\n",
            "tensor(0.0093) tensor(510) tensor(462)\n",
            "tensor(0.0041) tensor(311) tensor(640)\n",
            "tensor(0.0065) tensor(284) tensor(401)\n",
            "tensor(0.0053) tensor(624) tensor(493)\n",
            "tensor(0.0088) tensor(513) tensor(596)\n",
            "tensor(0.0038) tensor(455) tensor(631)\n",
            "tensor(0.0060) tensor(356) tensor(631)\n",
            "tensor(0.0073) tensor(544) tensor(306)\n",
            "tensor(0.0053) tensor(299) tensor(331)\n",
            "tensor(0.0032) tensor(567) tensor(587)\n",
            "tensor(0.0069) tensor(616) tensor(586)\n",
            "tensor(0.0072) tensor(587) tensor(559)\n",
            "tensor(0.0044) tensor(68) tensor(640)\n",
            "tensor(0.0022) tensor(618) tensor(625)\n",
            "tensor(0.0068) tensor(348) tensor(626)\n",
            "tensor(0.0018) tensor(606) tensor(622)\n",
            "tensor(2.9132e-05) tensor(627) tensor(507)\n",
            "tensor(0.0023) tensor(631) tensor(186)\n",
            "tensor(0.0056) tensor(641) tensor(534)\n",
            "tensor(0.0082) tensor(282) tensor(17)\n",
            "tensor(0.0007) tensor(287) tensor(631)\n",
            "tensor(0.0072) tensor(622) tensor(531)\n",
            "tensor(0.0005) tensor(303) tensor(642)\n",
            "tensor(0.0072) tensor(531) tensor(587)\n",
            "tensor(0.0075) tensor(218) tensor(541)\n",
            "tensor(0.0032) tensor(298) tensor(644)\n",
            "tensor(0.0094) tensor(283) tensor(549)\n",
            "tensor(0.0065) tensor(566) tensor(299)\n",
            "tensor(0.0019) tensor(88) tensor(642)\n",
            "tensor(0.0094) tensor(442) tensor(414)\n",
            "tensor(5.6648e-05) tensor(627) tensor(228)\n",
            "tensor(0.0078) tensor(598) tensor(628)\n",
            "tensor(0.0021) tensor(295) tensor(644)\n",
            "tensor(0.0040) tensor(636) tensor(391)\n",
            "tensor(0.0041) tensor(622) tensor(468)\n",
            "tensor(0.0074) tensor(38) tensor(636)\n",
            "tensor(0.0004) tensor(642) tensor(64)\n",
            "tensor(0.0059) tensor(509) tensor(530)\n",
            "tensor(0.0034) tensor(587) tensor(576)\n",
            "tensor(0.0045) tensor(641) tensor(544)\n",
            "tensor(0.0023) tensor(563) tensor(639)\n",
            "tensor(0.0053) tensor(384) tensor(609)\n",
            "tensor(0.0086) tensor(592) tensor(306)\n",
            "tensor(0.0015) tensor(561) tensor(641)\n",
            "tensor(0.0100) tensor(611) tensor(589)\n",
            "tensor(0.0064) tensor(559) tensor(618)\n",
            "tensor(0.0050) tensor(573) tensor(633)\n",
            "tensor(0.0060) tensor(508) tensor(636)\n",
            "tensor(0.0098) tensor(497) tensor(546)\n",
            "tensor(0.0074) tensor(138) tensor(643)\n",
            "tensor(0.0063) tensor(283) tensor(618)\n",
            "tensor(0.0089) tensor(566) tensor(473)\n",
            "tensor(0.0059) tensor(630) tensor(466)\n",
            "tensor(0.0009) tensor(293) tensor(631)\n",
            "tensor(0.0024) tensor(584) tensor(630)\n",
            "tensor(0.0026) tensor(644) tensor(96)\n",
            "tensor(0.0070) tensor(447) tensor(534)\n",
            "tensor(0.0085) tensor(577) tensor(387)\n",
            "tensor(0.0029) tensor(227) tensor(644)\n",
            "tensor(0.0050) tensor(561) tensor(617)\n",
            "tensor(0.0009) tensor(495) tensor(642)\n",
            "tensor(0.0075) tensor(631) tensor(274)\n",
            "tensor(0.0072) tensor(64) tensor(614)\n",
            "tensor(0.0069) tensor(588) tensor(624)\n",
            "tensor(0.0018) tensor(626) tensor(644)\n",
            "tensor(0.0009) tensor(609) tensor(636)\n",
            "tensor(0.0080) tensor(577) tensor(578)\n",
            "tensor(0.0007) tensor(68) tensor(642)\n",
            "tensor(0.0054) tensor(443) tensor(493)\n",
            "tensor(0.0072) tensor(609) tensor(473)\n",
            "tensor(0.0076) tensor(441) tensor(620)\n",
            "tensor(0.0056) tensor(331) tensor(579)\n",
            "tensor(0.0027) tensor(596) tensor(515)\n",
            "tensor(0.0084) tensor(5) tensor(410)\n",
            "tensor(0.0003) tensor(631) tensor(551)\n",
            "tensor(0.0015) tensor(643) tensor(487)\n",
            "tensor(0.0059) tensor(559) tensor(601)\n",
            "tensor(0.0088) tensor(571) tensor(555)\n",
            "tensor(8.7687e-06) tensor(239) tensor(627)\n",
            "tensor(0.0023) tensor(631) tensor(214)\n",
            "tensor(0.0087) tensor(64) tensor(253)\n",
            "tensor(0.0077) tensor(0) tensor(282)\n",
            "tensor(0.0073) tensor(558) tensor(348)\n",
            "tensor(0.0095) tensor(538) tensor(406)\n",
            "tensor(4.4368e-05) tensor(627) tensor(446)\n",
            "tensor(0.0031) tensor(642) tensor(4)\n",
            "tensor(0.0096) tensor(517) tensor(639)\n",
            "tensor(0.0092) tensor(363) tensor(639)\n",
            "tensor(0.0061) tensor(475) tensor(572)\n",
            "tensor(0.0065) tensor(622) tensor(68)\n",
            "tensor(0.0016) tensor(207) tensor(631)\n",
            "tensor(0.0059) tensor(525) tensor(640)\n",
            "tensor(0.0054) tensor(633) tensor(589)\n",
            "tensor(0.0062) tensor(563) tensor(384)\n",
            "tensor(0.0070) tensor(465) tensor(544)\n",
            "tensor(0.0074) tensor(541) tensor(494)\n",
            "tensor(0.0067) tensor(631) tensor(403)\n",
            "tensor(0.0038) tensor(609) tensor(613)\n",
            "tensor(0.0049) tensor(618) tensor(418)\n",
            "tensor(4.1068e-05) tensor(627) tensor(207)\n",
            "tensor(0.0087) tensor(96) tensor(620)\n",
            "tensor(0.0009) tensor(642) tensor(450)\n",
            "tensor(0.0091) tensor(502) tensor(541)\n",
            "tensor(0.0018) tensor(643) tensor(593)\n",
            "tensor(0.0039) tensor(191) tensor(636)\n",
            "tensor(0.0052) tensor(369) tensor(636)\n",
            "tensor(0.0098) tensor(578) tensor(469)\n",
            "tensor(0.0088) tensor(553) tensor(625)\n",
            "tensor(0.0018) tensor(631) tensor(273)\n",
            "tensor(0.0082) tensor(573) tensor(93)\n",
            "tensor(0.0074) tensor(515) tensor(266)\n",
            "tensor(0.0088) tensor(570) tensor(507)\n",
            "tensor(0.0011) tensor(338) tensor(631)\n",
            "tensor(0.0079) tensor(324) tensor(410)\n",
            "tensor(0.0091) tensor(438) tensor(540)\n",
            "tensor(0.0070) tensor(308) tensor(589)\n",
            "tensor(0.0003) tensor(622) tensor(643)\n",
            "tensor(0.0031) tensor(574) tensor(447)\n",
            "tensor(0.0075) tensor(630) tensor(453)\n",
            "tensor(2.3048e-05) tensor(631) tensor(642)\n",
            "tensor(0.0060) tensor(632) tensor(507)\n",
            "tensor(0.0056) tensor(127) tensor(630)\n",
            "tensor(0.0060) tensor(603) tensor(530)\n",
            "tensor(0.0100) tensor(540) tensor(553)\n",
            "tensor(0.0081) tensor(524) tensor(64)\n",
            "tensor(0.0038) tensor(620) tensor(561)\n",
            "tensor(0.0029) tensor(282) tensor(586)\n",
            "tensor(0.0056) tensor(493) tensor(549)\n",
            "tensor(0.0028) tensor(620) tensor(284)\n",
            "tensor(0.0027) tensor(643) tensor(265)\n",
            "tensor(0.0026) tensor(426) tensor(546)\n",
            "tensor(0.0067) tensor(553) tensor(572)\n",
            "tensor(0.0059) tensor(622) tensor(359)\n",
            "tensor(0.0063) tensor(551) tensor(549)\n",
            "tensor(0.0041) tensor(642) tensor(373)\n",
            "tensor(0.0090) tensor(551) tensor(584)\n",
            "tensor(0.0077) tensor(513) tensor(447)\n",
            "tensor(0.0047) tensor(491) tensor(601)\n",
            "tensor(0.0074) tensor(438) tensor(639)\n",
            "tensor(0.0058) tensor(588) tensor(613)\n",
            "tensor(0.0025) tensor(396) tensor(644)\n",
            "tensor(0.0076) tensor(438) tensor(628)\n",
            "tensor(0.0023) tensor(367) tensor(644)\n",
            "tensor(0.0096) tensor(567) tensor(488)\n",
            "tensor(0.0059) tensor(331) tensor(582)\n",
            "tensor(0.0066) tensor(447) tensor(520)\n",
            "tensor(0.0084) tensor(0) tensor(509)\n",
            "tensor(0.0053) tensor(306) tensor(507)\n",
            "tensor(0.0012) tensor(622) tensor(587)\n",
            "tensor(0.0054) tensor(442) tensor(282)\n",
            "tensor(0.0083) tensor(470) tensor(183)\n",
            "tensor(0.0068) tensor(418) tensor(541)\n",
            "tensor(0.0028) tensor(237) tensor(636)\n",
            "tensor(0.0033) tensor(631) tensor(139)\n",
            "tensor(0.0037) tensor(640) tensor(573)\n",
            "tensor(0.0088) tensor(538) tensor(617)\n",
            "tensor(0.0026) tensor(642) tensor(428)\n",
            "tensor(0.0044) tensor(493) tensor(282)\n",
            "tensor(0.0098) tensor(390) tensor(622)\n",
            "tensor(0.0093) tensor(406) tensor(507)\n",
            "tensor(4.4239e-05) tensor(627) tensor(258)\n",
            "tensor(0.0081) tensor(484) tensor(587)\n",
            "tensor(0.0003) tensor(631) tensor(546)\n",
            "tensor(0.0059) tensor(493) tensor(609)\n",
            "tensor(0.0049) tensor(474) tensor(284)\n",
            "tensor(0.0005) tensor(341) tensor(631)\n",
            "tensor(0.0077) tensor(632) tensor(570)\n",
            "tensor(0.0085) tensor(64) tensor(468)\n",
            "tensor(0.0050) tensor(449) tensor(622)\n",
            "tensor(0.0086) tensor(286) tensor(284)\n",
            "tensor(0.0051) tensor(613) tensor(579)\n",
            "tensor(0.0060) tensor(502) tensor(639)\n",
            "tensor(0.0091) tensor(630) tensor(257)\n",
            "tensor(0.0060) tensor(588) tensor(563)\n",
            "tensor(0.0003) tensor(530) tensor(642)\n",
            "tensor(0.0011) tensor(82) tensor(642)\n",
            "tensor(0.0011) tensor(415) tensor(631)\n",
            "tensor(0.0038) tensor(606) tensor(515)\n",
            "tensor(0.0039) tensor(625) tensor(541)\n",
            "tensor(0.0040) tensor(644) tensor(381)\n",
            "tensor(0.0097) tensor(348) tensor(498)\n",
            "tensor(0.0032) tensor(546) tensor(515)\n",
            "tensor(0.0028) tensor(630) tensor(590)\n",
            "tensor(0.0081) tensor(442) tensor(462)\n",
            "tensor(0.0029) tensor(586) tensor(426)\n",
            "tensor(0.0044) tensor(551) tensor(613)\n",
            "tensor(0.0009) tensor(33) tensor(631)\n",
            "tensor(0.0078) tensor(603) tensor(299)\n",
            "tensor(0.0085) tensor(613) tensor(585)\n",
            "tensor(0.0003) tensor(624) tensor(642)\n",
            "tensor(0.0004) tensor(119) tensor(642)\n",
            "tensor(0.0043) tensor(414) tensor(614)\n",
            "tensor(0.0099) tensor(203) tensor(530)\n",
            "tensor(0.0096) tensor(569) tensor(331)\n",
            "tensor(0.0065) tensor(93) tensor(617)\n",
            "tensor(0.0096) tensor(628) tensor(370)\n",
            "tensor(0.0032) tensor(319) tensor(643)\n",
            "tensor(0.0004) tensor(563) tensor(643)\n",
            "tensor(0.0029) tensor(447) tensor(609)\n",
            "tensor(0.0027) tensor(284) tensor(546)\n",
            "tensor(0.0061) tensor(172) tensor(630)\n",
            "tensor(0.0086) tensor(569) tensor(540)\n",
            "tensor(0.0083) tensor(491) tensor(541)\n",
            "tensor(0.0002) tensor(642) tensor(635)\n",
            "tensor(0.0017) tensor(21) tensor(642)\n",
            "tensor(0.0092) tensor(99) tensor(631)\n",
            "tensor(0.0039) tensor(394) tensor(643)\n",
            "tensor(4.0618e-05) tensor(627) tensor(83)\n",
            "tensor(0.0043) tensor(641) tensor(164)\n",
            "tensor(0.0073) tensor(503) tensor(624)\n",
            "tensor(0.0050) tensor(636) tensor(336)\n",
            "tensor(0.0084) tensor(616) tensor(609)\n",
            "tensor(0.0091) tensor(85) tensor(640)\n",
            "tensor(0.0047) tensor(442) tensor(625)\n",
            "tensor(0.0065) tensor(644) tensor(85)\n",
            "tensor(5.1988e-05) tensor(627) tensor(311)\n",
            "tensor(0.0012) tensor(167) tensor(636)\n",
            "tensor(0.0067) tensor(641) tensor(182)\n",
            "tensor(0.0013) tensor(636) tensor(566)\n",
            "tensor(0.0094) tensor(382) tensor(570)\n",
            "tensor(0.0049) tensor(234) tensor(609)\n",
            "tensor(0.0025) tensor(324) tensor(426)\n",
            "tensor(0.0049) tensor(571) tensor(563)\n",
            "tensor(0.0063) tensor(494) tensor(469)\n",
            "tensor(0.0066) tensor(541) tensor(507)\n",
            "tensor(0.0033) tensor(563) tensor(586)\n",
            "tensor(0.0058) tensor(331) tensor(584)\n",
            "tensor(0.0054) tensor(581) tensor(635)\n",
            "tensor(0.0059) tensor(635) tensor(441)\n",
            "tensor(0.0044) tensor(641) tensor(591)\n",
            "tensor(0.0054) tensor(64) tensor(299)\n",
            "tensor(0.0026) tensor(549) tensor(465)\n",
            "tensor(0.0087) tensor(569) tensor(306)\n",
            "tensor(0.0035) tensor(622) tensor(530)\n",
            "tensor(0.0070) tensor(631) tensor(408)\n",
            "tensor(0.0076) tensor(303) tensor(563)\n",
            "tensor(0.0075) tensor(622) tensor(575)\n",
            "tensor(0.0052) tensor(629) tensor(469)\n",
            "tensor(0.0038) tensor(636) tensor(205)\n",
            "tensor(0.0066) tensor(641) tensor(381)\n",
            "tensor(9.4359e-06) tensor(24) tensor(627)\n",
            "tensor(0.0061) tensor(535) tensor(561)\n",
            "tensor(0.0070) tensor(624) tensor(468)\n",
            "tensor(0.0018) tensor(341) tensor(636)\n",
            "tensor(0.0098) tensor(575) tensor(633)\n",
            "tensor(0.0094) tensor(625) tensor(503)\n",
            "tensor(0.0085) tensor(622) tensor(240)\n",
            "tensor(0.0059) tensor(308) tensor(629)\n",
            "tensor(0.0072) tensor(643) tensor(361)\n",
            "tensor(0.0030) tensor(284) tensor(414)\n",
            "tensor(0.0048) tensor(563) tensor(634)\n",
            "tensor(0.0083) tensor(64) tensor(533)\n",
            "tensor(0.0090) tensor(635) tensor(292)\n",
            "tensor(0.0065) tensor(622) tensor(295)\n",
            "tensor(0.0042) tensor(611) tensor(426)\n",
            "tensor(0.0072) tensor(382) tensor(530)\n",
            "tensor(0.0035) tensor(625) tensor(443)\n",
            "tensor(0.0081) tensor(642) tensor(77)\n",
            "tensor(0.0039) tensor(624) tensor(64)\n",
            "tensor(0.0082) tensor(444) tensor(515)\n",
            "tensor(0.0060) tensor(280) tensor(636)\n",
            "tensor(6.5362e-06) tensor(68) tensor(627)\n",
            "tensor(0.0045) tensor(378) tensor(587)\n",
            "tensor(0.0048) tensor(306) tensor(634)\n",
            "tensor(0.0065) tensor(506) tensor(549)\n",
            "tensor(0.0032) tensor(574) tensor(596)\n",
            "tensor(0.0080) tensor(617) tensor(634)\n",
            "tensor(0.0041) tensor(250) tensor(640)\n",
            "tensor(0.0039) tensor(594) tensor(465)\n",
            "tensor(0.0050) tensor(546) tensor(308)\n",
            "tensor(0.0057) tensor(571) tensor(64)\n",
            "tensor(0.0027) tensor(638) tensor(639)\n",
            "tensor(0.0040) tensor(638) tensor(586)\n",
            "tensor(8.2858e-05) tensor(631) tensor(636)\n",
            "tensor(0.0088) tensor(545) tensor(540)\n",
            "tensor(0.0028) tensor(48) tensor(643)\n",
            "tensor(0.0070) tensor(640) tensor(599)\n",
            "tensor(0.0100) tensor(442) tensor(579)\n",
            "tensor(0.0079) tensor(331) tensor(570)\n",
            "tensor(0.0088) tensor(210) tensor(630)\n",
            "tensor(0.0032) tensor(577) tensor(299)\n",
            "tensor(0.0030) tensor(601) tensor(308)\n",
            "tensor(0.0034) tensor(596) tensor(586)\n",
            "tensor(0.0083) tensor(611) tensor(331)\n",
            "tensor(0.0055) tensor(632) tensor(606)\n",
            "tensor(0.0093) tensor(630) tensor(432)\n",
            "tensor(0.0061) tensor(644) tensor(270)\n",
            "tensor(0.0062) tensor(357) tensor(644)\n",
            "tensor(0.0056) tensor(85) tensor(636)\n",
            "tensor(0.0058) tensor(587) tensor(266)\n",
            "tensor(0.0085) tensor(626) tensor(632)\n",
            "tensor(0.0025) tensor(635) tensor(530)\n",
            "tensor(0.0089) tensor(602) tensor(282)\n",
            "tensor(0.0040) tensor(494) tensor(465)\n",
            "tensor(0.0093) tensor(242) tensor(635)\n",
            "tensor(8.6721e-06) tensor(367) tensor(627)\n",
            "tensor(0.0023) tensor(631) tensor(479)\n",
            "tensor(0.0042) tensor(618) tensor(507)\n",
            "tensor(0.0011) tensor(591) tensor(643)\n",
            "tensor(0.0075) tensor(600) tensor(632)\n",
            "tensor(0.0077) tensor(227) tensor(635)\n",
            "tensor(0.0078) tensor(210) tensor(641)\n",
            "tensor(0.0060) tensor(284) tensor(537)\n",
            "tensor(0.0004) tensor(642) tensor(549)\n",
            "tensor(0.0058) tensor(213) tensor(622)\n",
            "tensor(0.0051) tensor(190) tensor(631)\n",
            "tensor(0.0086) tensor(391) tensor(348)\n",
            "tensor(0.0082) tensor(341) tensor(530)\n",
            "tensor(0.0099) tensor(396) tensor(515)\n",
            "tensor(0.0020) tensor(587) tensor(624)\n",
            "tensor(0.0023) tensor(331) tensor(635)\n",
            "tensor(0.0038) tensor(640) tensor(594)\n",
            "tensor(0.0015) tensor(642) tensor(285)\n",
            "tensor(0.0083) tensor(550) tensor(157)\n",
            "tensor(0.0014) tensor(642) tensor(69)\n",
            "tensor(0.0063) tensor(470) tensor(469)\n",
            "tensor(0.0057) tensor(644) tensor(47)\n",
            "tensor(2.3521e-05) tensor(627) tensor(406)\n",
            "tensor(0.0007) tensor(572) tensor(641)\n",
            "tensor(0.0012) tensor(453) tensor(643)\n",
            "tensor(0.0008) tensor(324) tensor(636)\n",
            "tensor(0.0073) tensor(530) tensor(473)\n",
            "tensor(0.0086) tensor(454) tensor(509)\n",
            "tensor(0.0066) tensor(630) tensor(317)\n",
            "tensor(0.0062) tensor(441) tensor(284)\n",
            "tensor(0.0073) tensor(578) tensor(628)\n",
            "tensor(0.0071) tensor(623) tensor(582)\n",
            "tensor(0.0041) tensor(616) tensor(426)\n",
            "tensor(0.0069) tensor(438) tensor(572)\n",
            "tensor(0.0057) tensor(540) tensor(506)\n",
            "tensor(0.0076) tensor(551) tensor(331)\n",
            "tensor(0.0091) tensor(620) tensor(401)\n",
            "tensor(0.0019) tensor(306) tensor(587)\n",
            "tensor(0.0078) tensor(601) tensor(626)\n",
            "tensor(0.0022) tensor(576) tensor(630)\n",
            "tensor(0.0058) tensor(486) tensor(601)\n",
            "tensor(0.0054) tensor(628) tensor(382)\n",
            "tensor(0.0092) tensor(406) tensor(606)\n",
            "tensor(0.0059) tensor(630) tensor(439)\n",
            "tensor(4.2487e-05) tensor(627) tensor(56)\n",
            "tensor(0.0042) tensor(306) tensor(93)\n",
            "tensor(0.0083) tensor(426) tensor(621)\n",
            "tensor(0.0005) tensor(577) tensor(644)\n",
            "tensor(0.0058) tensor(592) tensor(587)\n",
            "tensor(0.0079) tensor(135) tensor(644)\n",
            "tensor(0.0027) tensor(642) tensor(202)\n",
            "tensor(0.0013) tensor(295) tensor(643)\n",
            "tensor(0.0056) tensor(540) tensor(418)\n",
            "tensor(0.0095) tensor(533) tensor(234)\n",
            "tensor(0.0088) tensor(597) tensor(308)\n",
            "tensor(0.0025) tensor(632) tensor(618)\n",
            "tensor(0.0058) tensor(599) tensor(635)\n",
            "tensor(0.0079) tensor(302) tensor(640)\n",
            "tensor(0.0011) tensor(551) tensor(636)\n",
            "tensor(0.0073) tensor(450) tensor(282)\n",
            "tensor(0.0099) tensor(292) tensor(515)\n",
            "tensor(0.0085) tensor(537) tensor(614)\n",
            "tensor(0.0079) tensor(561) tensor(405)\n",
            "tensor(0.0009) tensor(17) tensor(643)\n",
            "tensor(0.0017) tensor(512) tensor(644)\n",
            "tensor(0.0034) tensor(414) tensor(157)\n",
            "tensor(0.0059) tensor(630) tensor(612)\n",
            "tensor(0.0063) tensor(641) tensor(358)\n",
            "tensor(0.0064) tensor(553) tensor(601)\n",
            "tensor(0.0076) tensor(576) tensor(183)\n",
            "tensor(0.0096) tensor(466) tensor(613)\n",
            "tensor(0.0075) tensor(570) tensor(586)\n",
            "tensor(0.0052) tensor(447) tensor(604)\n",
            "tensor(3.4540e-06) tensor(627) tensor(642)\n",
            "tensor(0.0047) tensor(641) tensor(592)\n",
            "tensor(0.0067) tensor(604) tensor(638)\n",
            "tensor(0.0048) tensor(164) tensor(635)\n",
            "tensor(0.0042) tensor(93) tensor(596)\n",
            "tensor(2.8599e-06) tensor(627) tensor(414)\n",
            "tensor(0.0024) tensor(642) tensor(365)\n",
            "tensor(0.0035) tensor(384) tensor(628)\n",
            "tensor(0.0057) tensor(578) tensor(587)\n",
            "tensor(0.0027) tensor(543) tensor(636)\n",
            "tensor(0.0088) tensor(171) tensor(628)\n",
            "tensor(0.0093) tensor(316) tensor(587)\n",
            "tensor(0.0014) tensor(306) tensor(622)\n",
            "tensor(0.0095) tensor(567) tensor(611)\n",
            "tensor(0.0072) tensor(64) tensor(512)\n",
            "tensor(0.0039) tensor(587) tensor(488)\n",
            "tensor(0.0033) tensor(103) tensor(644)\n",
            "tensor(5.0799e-06) tensor(465) tensor(627)\n",
            "tensor(0.0040) tensor(643) tensor(485)\n",
            "tensor(0.0081) tensor(625) tensor(401)\n",
            "tensor(0.0056) tensor(640) tensor(591)\n",
            "tensor(4.4591e-05) tensor(627) tensor(615)\n",
            "tensor(0.0096) tensor(625) tensor(600)\n",
            "tensor(0.0065) tensor(444) tensor(601)\n",
            "tensor(0.0074) tensor(237) tensor(465)\n",
            "tensor(0.0073) tensor(541) tensor(588)\n",
            "tensor(5.2877e-05) tensor(627) tensor(44)\n",
            "tensor(0.0048) tensor(17) tensor(618)\n",
            "tensor(0.0072) tensor(561) tensor(603)\n",
            "tensor(0.0023) tensor(596) tensor(618)\n",
            "tensor(0.0040) tensor(628) tensor(620)\n",
            "tensor(0.0092) tensor(599) tensor(624)\n",
            "tensor(0.0039) tensor(494) tensor(348)\n",
            "tensor(0.0036) tensor(641) tensor(401)\n",
            "tensor(3.6667e-05) tensor(627) tensor(263)\n",
            "tensor(0.0042) tensor(522) tensor(622)\n",
            "tensor(0.0009) tensor(240) tensor(631)\n",
            "tensor(0.0080) tensor(583) tensor(462)\n",
            "tensor(0.0064) tensor(504) tensor(348)\n",
            "tensor(2.4934e-06) tensor(596) tensor(627)\n",
            "tensor(0.0094) tensor(567) tensor(616)\n",
            "tensor(0.0026) tensor(630) tensor(632)\n",
            "tensor(4.7414e-05) tensor(627) tensor(436)\n",
            "tensor(0.0009) tensor(636) tensor(549)\n",
            "tensor(0.0045) tensor(606) tensor(596)\n",
            "tensor(0.0073) tensor(640) tensor(564)\n",
            "tensor(0.0095) tensor(560) tensor(613)\n",
            "tensor(0.0021) tensor(395) tensor(644)\n",
            "tensor(0.0033) tensor(249) tensor(641)\n",
            "tensor(0.0036) tensor(183) tensor(635)\n",
            "tensor(0.0094) tensor(635) tensor(542)\n",
            "tensor(0.0018) tensor(549) tensor(630)\n",
            "tensor(0.0010) tensor(641) tensor(587)\n",
            "tensor(0.0051) tensor(406) tensor(596)\n",
            "tensor(0.0034) tensor(348) tensor(571)\n",
            "tensor(0.0031) tensor(165) tensor(642)\n",
            "tensor(0.0004) tensor(631) tensor(623)\n",
            "tensor(0.0004) tensor(510) tensor(631)\n",
            "tensor(0.0083) tensor(542) tensor(348)\n",
            "tensor(0.0069) tensor(625) tensor(520)\n",
            "tensor(0.0040) tensor(576) tensor(628)\n",
            "tensor(0.0022) tensor(415) tensor(643)\n",
            "tensor(0.0037) tensor(622) tensor(603)\n",
            "tensor(0.0075) tensor(171) tensor(465)\n",
            "tensor(0.0018) tensor(639) tensor(628)\n",
            "tensor(0.0037) tensor(580) tensor(641)\n",
            "tensor(0.0093) tensor(308) tensor(505)\n",
            "tensor(0.0050) tensor(630) tensor(410)\n",
            "tensor(0.0085) tensor(403) tensor(641)\n",
            "tensor(0.0046) tensor(462) tensor(563)\n",
            "tensor(2.8143e-05) tensor(627) tensor(246)\n",
            "tensor(0.0056) tensor(5) tensor(443)\n",
            "tensor(7.3861e-06) tensor(169) tensor(627)\n",
            "tensor(0.0071) tensor(476) tensor(540)\n",
            "tensor(0.0073) tensor(306) tensor(505)\n",
            "tensor(0.0039) tensor(636) tensor(232)\n",
            "tensor(0.0063) tensor(466) tensor(348)\n",
            "tensor(0.0013) tensor(310) tensor(642)\n",
            "tensor(0.0045) tensor(299) tensor(563)\n",
            "tensor(0.0045) tensor(641) tensor(527)\n",
            "tensor(0.0096) tensor(250) tensor(638)\n",
            "tensor(0.0006) tensor(642) tensor(614)\n",
            "tensor(0.0049) tensor(5) tensor(638)\n",
            "tensor(0.0069) tensor(261) tensor(636)\n",
            "tensor(0.0005) tensor(643) tensor(306)\n",
            "tensor(0.0038) tensor(639) tensor(493)\n",
            "tensor(0.0070) tensor(234) tensor(634)\n",
            "tensor(7.5694e-06) tensor(580) tensor(627)\n",
            "tensor(0.0060) tensor(633) tensor(551)\n",
            "tensor(0.0068) tensor(618) tensor(491)\n",
            "tensor(0.0092) tensor(170) tensor(643)\n",
            "tensor(0.0079) tensor(524) tensor(541)\n",
            "tensor(0.0026) tensor(601) tensor(443)\n",
            "tensor(0.0037) tensor(226) tensor(644)\n",
            "tensor(0.0060) tensor(466) tensor(587)\n",
            "tensor(0.0081) tensor(223) tensor(644)\n",
            "tensor(0.0060) tensor(574) tensor(588)\n",
            "tensor(0.0024) tensor(587) tensor(570)\n",
            "tensor(0.0007) tensor(456) tensor(642)\n",
            "tensor(0.0083) tensor(387) tensor(306)\n",
            "tensor(0.0081) tensor(370) tensor(447)\n",
            "tensor(0.0020) tensor(545) tensor(644)\n",
            "tensor(0.0034) tensor(589) tensor(348)\n",
            "tensor(0.0027) tensor(635) tensor(308)\n",
            "tensor(0.0056) tensor(505) tensor(426)\n",
            "tensor(0.0025) tensor(624) tensor(284)\n",
            "tensor(0.0002) tensor(642) tensor(622)\n",
            "tensor(0.0046) tensor(299) tensor(624)\n",
            "tensor(0.0041) tensor(540) tensor(634)\n",
            "tensor(0.0047) tensor(439) tensor(635)\n",
            "tensor(0.0056) tensor(636) tensor(102)\n",
            "tensor(0.0011) tensor(606) tensor(636)\n",
            "tensor(0.0037) tensor(642) tensor(49)\n",
            "tensor(0.0020) tensor(284) tensor(577)\n",
            "tensor(0.0093) tensor(283) tensor(586)\n",
            "tensor(0.0008) tensor(414) tensor(641)\n",
            "tensor(0.0069) tensor(569) tensor(628)\n",
            "tensor(0.0023) tensor(639) tensor(632)\n",
            "tensor(8.2669e-06) tensor(382) tensor(627)\n",
            "tensor(0.0034) tensor(640) tensor(584)\n",
            "tensor(0.0071) tensor(637) tensor(624)\n",
            "tensor(0.0076) tensor(624) tensor(410)\n",
            "tensor(0.0013) tensor(631) tensor(172)\n",
            "tensor(0.0081) tensor(635) tensor(481)\n",
            "tensor(0.0060) tensor(601) tensor(266)\n",
            "tensor(0.0084) tensor(609) tensor(583)\n",
            "tensor(0.0006) tensor(631) tensor(468)\n",
            "tensor(0.0051) tensor(644) tensor(83)\n",
            "tensor(0.0049) tensor(524) tensor(639)\n",
            "tensor(0.0007) tensor(612) tensor(631)\n",
            "tensor(0.0099) tensor(367) tensor(624)\n",
            "tensor(0.0085) tensor(641) tensor(262)\n",
            "tensor(0.0018) tensor(119) tensor(641)\n",
            "tensor(0.0041) tensor(324) tensor(632)\n",
            "tensor(0.0028) tensor(391) tensor(644)\n",
            "tensor(0.0092) tensor(548) tensor(613)\n",
            "tensor(0.0050) tensor(549) tensor(299)\n",
            "tensor(0.0056) tensor(183) tensor(601)\n",
            "tensor(0.0014) tensor(630) tensor(447)\n",
            "tensor(0.0063) tensor(640) tensor(311)\n",
            "tensor(0.0056) tensor(401) tensor(284)\n",
            "tensor(0.0047) tensor(493) tensor(624)\n",
            "tensor(0.0057) tensor(631) tensor(455)\n",
            "tensor(0.0089) tensor(631) tensor(356)\n",
            "tensor(0.0081) tensor(306) tensor(544)\n",
            "tensor(0.0056) tensor(331) tensor(299)\n",
            "tensor(0.0035) tensor(587) tensor(567)\n",
            "tensor(0.0068) tensor(586) tensor(616)\n",
            "tensor(0.0057) tensor(559) tensor(587)\n",
            "tensor(0.0064) tensor(640) tensor(68)\n",
            "tensor(0.0024) tensor(625) tensor(618)\n",
            "tensor(0.0055) tensor(626) tensor(348)\n",
            "tensor(0.0027) tensor(622) tensor(606)\n",
            "tensor(5.1364e-06) tensor(507) tensor(627)\n",
            "tensor(0.0017) tensor(186) tensor(631)\n",
            "tensor(0.0089) tensor(590) tensor(619)\n",
            "tensor(0.0040) tensor(534) tensor(641)\n",
            "tensor(0.0061) tensor(17) tensor(282)\n",
            "tensor(0.0009) tensor(631) tensor(287)\n",
            "tensor(0.0051) tensor(531) tensor(622)\n",
            "tensor(0.0008) tensor(642) tensor(303)\n",
            "tensor(0.0093) tensor(587) tensor(531)\n",
            "tensor(0.0087) tensor(541) tensor(218)\n",
            "tensor(0.0046) tensor(644) tensor(298)\n",
            "tensor(0.0069) tensor(299) tensor(566)\n",
            "tensor(0.0030) tensor(642) tensor(88)\n",
            "tensor(0.0096) tensor(593) tensor(540)\n",
            "tensor(8.4328e-06) tensor(228) tensor(627)\n",
            "tensor(0.0032) tensor(644) tensor(295)\n",
            "tensor(0.0096) tensor(437) tensor(640)\n",
            "tensor(0.0031) tensor(391) tensor(636)\n",
            "tensor(0.0029) tensor(468) tensor(622)\n",
            "tensor(0.0098) tensor(636) tensor(38)\n",
            "tensor(0.0003) tensor(64) tensor(642)\n",
            "tensor(0.0062) tensor(530) tensor(509)\n",
            "tensor(0.0095) tensor(384) tensor(473)\n",
            "tensor(0.0031) tensor(576) tensor(587)\n",
            "tensor(0.0030) tensor(544) tensor(641)\n",
            "tensor(0.0026) tensor(639) tensor(563)\n",
            "tensor(0.0072) tensor(609) tensor(384)\n",
            "tensor(0.0016) tensor(641) tensor(561)\n",
            "tensor(0.0088) tensor(589) tensor(611)\n",
            "tensor(0.0085) tensor(450) tensor(620)\n",
            "tensor(0.0081) tensor(618) tensor(559)\n",
            "tensor(0.0062) tensor(633) tensor(573)\n",
            "tensor(0.0076) tensor(636) tensor(508)\n",
            "tensor(0.0085) tensor(618) tensor(283)\n",
            "tensor(0.0086) tensor(473) tensor(566)\n",
            "tensor(0.0072) tensor(175) tensor(622)\n",
            "tensor(0.0040) tensor(466) tensor(630)\n",
            "tensor(0.0012) tensor(631) tensor(293)\n",
            "tensor(0.0033) tensor(630) tensor(584)\n",
            "tensor(0.0018) tensor(96) tensor(644)\n",
            "tensor(0.0069) tensor(534) tensor(447)\n",
            "tensor(0.0086) tensor(387) tensor(632)\n",
            "tensor(0.0069) tensor(387) tensor(577)\n",
            "tensor(0.0041) tensor(644) tensor(227)\n",
            "tensor(0.0045) tensor(617) tensor(561)\n",
            "tensor(0.0013) tensor(642) tensor(495)\n",
            "tensor(0.0050) tensor(274) tensor(631)\n",
            "tensor(0.0094) tensor(210) tensor(622)\n",
            "tensor(0.0092) tensor(205) tensor(639)\n",
            "tensor(0.0074) tensor(614) tensor(64)\n",
            "tensor(0.0076) tensor(624) tensor(588)\n",
            "tensor(0.0085) tensor(492) tensor(348)\n",
            "tensor(0.0076) tensor(182) tensor(635)\n",
            "tensor(0.0028) tensor(644) tensor(626)\n",
            "tensor(0.0010) tensor(636) tensor(609)\n",
            "tensor(0.0069) tensor(578) tensor(577)\n",
            "tensor(0.0011) tensor(642) tensor(68)\n",
            "tensor(0.0057) tensor(493) tensor(443)\n",
            "tensor(0.0065) tensor(473) tensor(609)\n",
            "tensor(0.0046) tensor(579) tensor(331)\n",
            "tensor(0.0026) tensor(515) tensor(596)\n",
            "tensor(0.0093) tensor(497) tensor(586)\n",
            "tensor(0.0003) tensor(551) tensor(631)\n",
            "tensor(0.0012) tensor(487) tensor(643)\n",
            "tensor(0.0078) tensor(626) tensor(625)\n",
            "tensor(0.0080) tensor(601) tensor(559)\n",
            "tensor(0.0074) tensor(555) tensor(571)\n",
            "tensor(5.2845e-05) tensor(627) tensor(239)\n",
            "tensor(0.0017) tensor(214) tensor(631)\n",
            "tensor(0.0082) tensor(253) tensor(64)\n",
            "tensor(0.0091) tensor(282) tensor(0)\n",
            "tensor(0.0097) tensor(348) tensor(558)\n",
            "tensor(0.0085) tensor(406) tensor(538)\n",
            "tensor(7.6158e-06) tensor(446) tensor(627)\n",
            "tensor(0.0020) tensor(4) tensor(642)\n",
            "tensor(0.0074) tensor(572) tensor(475)\n",
            "tensor(0.0045) tensor(68) tensor(622)\n",
            "tensor(0.0087) tensor(230) tensor(643)\n",
            "tensor(0.0022) tensor(631) tensor(207)\n",
            "tensor(0.0082) tensor(640) tensor(525)\n",
            "tensor(0.0043) tensor(589) tensor(633)\n",
            "tensor(0.0052) tensor(384) tensor(563)\n",
            "tensor(0.0054) tensor(544) tensor(465)\n",
            "tensor(0.0069) tensor(494) tensor(541)\n",
            "tensor(0.0064) tensor(403) tensor(631)\n",
            "tensor(0.0034) tensor(613) tensor(609)\n",
            "tensor(0.0043) tensor(418) tensor(618)\n",
            "tensor(6.7934e-06) tensor(207) tensor(627)\n",
            "tensor(0.0006) tensor(450) tensor(642)\n",
            "tensor(0.0014) tensor(593) tensor(643)\n",
            "tensor(0.0050) tensor(636) tensor(191)\n",
            "tensor(0.0066) tensor(636) tensor(369)\n",
            "tensor(0.0092) tensor(469) tensor(578)\n",
            "tensor(0.0095) tensor(384) tensor(589)\n",
            "tensor(0.0014) tensor(273) tensor(631)\n",
            "tensor(0.0089) tensor(589) tensor(512)\n",
            "tensor(0.0065) tensor(459) tensor(644)\n",
            "tensor(0.0090) tensor(93) tensor(573)\n",
            "tensor(0.0058) tensor(266) tensor(515)\n",
            "tensor(0.0061) tensor(507) tensor(570)\n",
            "tensor(0.0015) tensor(631) tensor(338)\n",
            "tensor(0.0072) tensor(410) tensor(324)\n",
            "tensor(0.0070) tensor(589) tensor(308)\n",
            "tensor(0.0002) tensor(643) tensor(622)\n",
            "tensor(0.0082) tensor(302) tensor(622)\n",
            "tensor(0.0098) tensor(481) tensor(282)\n",
            "tensor(0.0040) tensor(447) tensor(574)\n",
            "tensor(0.0048) tensor(453) tensor(630)\n",
            "tensor(3.0157e-05) tensor(642) tensor(631)\n",
            "tensor(0.0052) tensor(507) tensor(632)\n",
            "tensor(0.0080) tensor(630) tensor(127)\n",
            "tensor(0.0085) tensor(530) tensor(603)\n",
            "tensor(0.0087) tensor(553) tensor(540)\n",
            "tensor(0.0068) tensor(36) tensor(642)\n",
            "tensor(0.0090) tensor(64) tensor(524)\n",
            "tensor(0.0039) tensor(561) tensor(620)\n",
            "tensor(0.0033) tensor(586) tensor(282)\n",
            "tensor(0.0059) tensor(549) tensor(493)\n",
            "tensor(0.0025) tensor(284) tensor(620)\n",
            "tensor(0.0021) tensor(265) tensor(643)\n",
            "tensor(0.0088) tensor(232) tensor(572)\n",
            "tensor(0.0031) tensor(546) tensor(426)\n",
            "tensor(0.0077) tensor(572) tensor(553)\n",
            "tensor(0.0041) tensor(359) tensor(622)\n",
            "tensor(0.0057) tensor(549) tensor(551)\n",
            "tensor(0.0027) tensor(373) tensor(642)\n",
            "tensor(0.0080) tensor(584) tensor(551)\n",
            "tensor(0.0097) tensor(378) tensor(308)\n",
            "tensor(0.0085) tensor(447) tensor(513)\n",
            "tensor(0.0065) tensor(601) tensor(491)\n",
            "tensor(0.0093) tensor(639) tensor(438)\n",
            "tensor(0.0057) tensor(613) tensor(588)\n",
            "tensor(0.0036) tensor(644) tensor(396)\n",
            "tensor(0.0035) tensor(644) tensor(367)\n",
            "tensor(0.0095) tensor(488) tensor(567)\n",
            "tensor(0.0055) tensor(582) tensor(331)\n",
            "tensor(0.0052) tensor(520) tensor(447)\n",
            "tensor(0.0048) tensor(507) tensor(306)\n",
            "tensor(0.0011) tensor(587) tensor(622)\n",
            "tensor(0.0073) tensor(282) tensor(442)\n",
            "tensor(0.0099) tensor(183) tensor(470)\n",
            "tensor(0.0077) tensor(541) tensor(418)\n",
            "tensor(0.0037) tensor(636) tensor(237)\n",
            "tensor(0.0023) tensor(139) tensor(631)\n",
            "tensor(0.0026) tensor(573) tensor(640)\n",
            "tensor(0.0082) tensor(617) tensor(538)\n",
            "tensor(0.0016) tensor(428) tensor(642)\n",
            "tensor(0.0071) tensor(473) tensor(570)\n",
            "tensor(0.0044) tensor(282) tensor(493)\n",
            "tensor(0.0096) tensor(507) tensor(406)\n",
            "tensor(7.1576e-06) tensor(258) tensor(627)\n",
            "tensor(0.0003) tensor(546) tensor(631)\n",
            "tensor(0.0064) tensor(609) tensor(493)\n",
            "tensor(0.0058) tensor(284) tensor(474)\n",
            "tensor(0.0007) tensor(631) tensor(341)\n",
            "tensor(0.0086) tensor(498) tensor(628)\n",
            "tensor(0.0086) tensor(570) tensor(632)\n",
            "tensor(0.0074) tensor(468) tensor(64)\n",
            "tensor(0.0060) tensor(333) tensor(642)\n",
            "tensor(0.0072) tensor(622) tensor(449)\n",
            "tensor(0.0045) tensor(579) tensor(613)\n",
            "tensor(0.0073) tensor(639) tensor(502)\n",
            "tensor(0.0063) tensor(257) tensor(630)\n",
            "tensor(0.0057) tensor(563) tensor(588)\n",
            "tensor(0.0004) tensor(642) tensor(530)\n",
            "tensor(0.0017) tensor(642) tensor(82)\n",
            "tensor(0.0015) tensor(631) tensor(415)\n",
            "tensor(0.0046) tensor(515) tensor(606)\n",
            "tensor(0.0036) tensor(541) tensor(625)\n",
            "tensor(0.0027) tensor(381) tensor(644)\n",
            "tensor(0.0074) tensor(498) tensor(348)\n",
            "tensor(0.0029) tensor(515) tensor(546)\n",
            "tensor(0.0020) tensor(590) tensor(630)\n",
            "tensor(0.0025) tensor(426) tensor(586)\n",
            "tensor(0.0087) tensor(341) tensor(541)\n",
            "tensor(0.0039) tensor(613) tensor(551)\n",
            "tensor(0.0012) tensor(631) tensor(33)\n",
            "tensor(0.0078) tensor(585) tensor(613)\n",
            "tensor(0.0003) tensor(642) tensor(624)\n",
            "tensor(0.0006) tensor(642) tensor(119)\n",
            "tensor(0.0059) tensor(614) tensor(414)\n",
            "tensor(0.0066) tensor(617) tensor(93)\n",
            "tensor(0.0070) tensor(370) tensor(628)\n",
            "tensor(0.0040) tensor(643) tensor(319)\n",
            "tensor(0.0005) tensor(643) tensor(563)\n",
            "tensor(0.0034) tensor(609) tensor(447)\n",
            "tensor(0.0031) tensor(546) tensor(284)\n",
            "tensor(0.0087) tensor(630) tensor(172)\n",
            "tensor(0.0098) tensor(381) tensor(628)\n",
            "tensor(0.0002) tensor(635) tensor(642)\n",
            "tensor(0.0027) tensor(642) tensor(21)\n",
            "tensor(0.0052) tensor(643) tensor(394)\n",
            "tensor(0.0098) tensor(162) tensor(465)\n",
            "tensor(6.0690e-06) tensor(83) tensor(627)\n",
            "tensor(0.0031) tensor(164) tensor(641)\n",
            "tensor(0.0091) tensor(624) tensor(503)\n",
            "tensor(0.0039) tensor(336) tensor(636)\n",
            "tensor(0.0087) tensor(269) tensor(631)\n",
            "tensor(0.0083) tensor(609) tensor(616)\n",
            "tensor(0.0072) tensor(625) tensor(442)\n",
            "tensor(0.0044) tensor(85) tensor(644)\n",
            "tensor(9.6105e-06) tensor(311) tensor(627)\n",
            "tensor(0.0015) tensor(636) tensor(167)\n",
            "tensor(0.0048) tensor(182) tensor(641)\n",
            "tensor(0.0011) tensor(566) tensor(636)\n",
            "tensor(0.0056) tensor(609) tensor(234)\n",
            "tensor(0.0020) tensor(426) tensor(324)\n",
            "tensor(0.0046) tensor(563) tensor(571)\n",
            "tensor(0.0052) tensor(469) tensor(494)\n",
            "tensor(0.0059) tensor(507) tensor(541)\n",
            "tensor(0.0038) tensor(586) tensor(563)\n",
            "tensor(0.0053) tensor(584) tensor(331)\n",
            "tensor(0.0070) tensor(635) tensor(581)\n",
            "tensor(0.0042) tensor(441) tensor(635)\n",
            "tensor(0.0031) tensor(591) tensor(641)\n",
            "tensor(0.0054) tensor(299) tensor(64)\n",
            "tensor(0.0027) tensor(465) tensor(549)\n",
            "tensor(0.0032) tensor(530) tensor(622)\n",
            "tensor(0.0047) tensor(408) tensor(631)\n",
            "tensor(0.0086) tensor(563) tensor(303)\n",
            "tensor(0.0052) tensor(575) tensor(622)\n",
            "tensor(0.0043) tensor(469) tensor(629)\n",
            "tensor(0.0030) tensor(205) tensor(636)\n",
            "tensor(0.0047) tensor(381) tensor(641)\n",
            "tensor(5.4766e-05) tensor(627) tensor(24)\n",
            "tensor(0.0071) tensor(561) tensor(535)\n",
            "tensor(0.0058) tensor(468) tensor(624)\n",
            "tensor(0.0024) tensor(636) tensor(341)\n",
            "tensor(0.0086) tensor(395) tensor(515)\n",
            "tensor(0.0074) tensor(503) tensor(625)\n",
            "tensor(0.0060) tensor(240) tensor(622)\n",
            "tensor(0.0062) tensor(629) tensor(308)\n",
            "tensor(0.0052) tensor(361) tensor(643)\n",
            "tensor(0.0022) tensor(414) tensor(284)\n",
            "tensor(0.0054) tensor(634) tensor(563)\n",
            "tensor(0.0080) tensor(533) tensor(64)\n",
            "tensor(0.0067) tensor(292) tensor(635)\n",
            "tensor(0.0045) tensor(295) tensor(622)\n",
            "tensor(0.0038) tensor(426) tensor(611)\n",
            "tensor(0.0081) tensor(530) tensor(382)\n",
            "tensor(0.0028) tensor(443) tensor(625)\n",
            "tensor(0.0042) tensor(77) tensor(642)\n",
            "tensor(0.0037) tensor(64) tensor(624)\n",
            "tensor(0.0097) tensor(515) tensor(444)\n",
            "tensor(0.0075) tensor(636) tensor(280)\n",
            "tensor(3.9758e-05) tensor(627) tensor(68)\n",
            "tensor(0.0058) tensor(587) tensor(378)\n",
            "tensor(0.0050) tensor(634) tensor(306)\n",
            "tensor(0.0074) tensor(549) tensor(506)\n",
            "tensor(0.0046) tensor(596) tensor(574)\n",
            "tensor(0.0086) tensor(634) tensor(617)\n",
            "tensor(0.0089) tensor(529) tensor(561)\n",
            "tensor(0.0062) tensor(640) tensor(250)\n",
            "tensor(0.0050) tensor(465) tensor(594)\n",
            "tensor(0.0040) tensor(308) tensor(546)\n",
            "tensor(0.0059) tensor(64) tensor(571)\n",
            "tensor(0.0025) tensor(639) tensor(638)\n",
            "tensor(0.0040) tensor(586) tensor(638)\n",
            "tensor(6.8565e-05) tensor(636) tensor(631)\n",
            "tensor(0.0036) tensor(643) tensor(48)\n",
            "tensor(0.0045) tensor(599) tensor(640)\n",
            "tensor(0.0095) tensor(212) tensor(641)\n",
            "tensor(0.0090) tensor(570) tensor(331)\n",
            "tensor(0.0034) tensor(299) tensor(577)\n",
            "tensor(0.0022) tensor(308) tensor(601)\n",
            "tensor(0.0036) tensor(586) tensor(596)\n",
            "tensor(0.0087) tensor(331) tensor(611)\n",
            "tensor(0.0041) tensor(606) tensor(632)\n",
            "tensor(0.0065) tensor(432) tensor(630)\n",
            "tensor(0.0042) tensor(270) tensor(644)\n",
            "tensor(0.0077) tensor(602) tensor(628)\n",
            "tensor(0.0091) tensor(644) tensor(357)\n",
            "tensor(0.0071) tensor(636) tensor(85)\n",
            "tensor(0.0043) tensor(266) tensor(587)\n",
            "tensor(0.0092) tensor(475) tensor(549)\n",
            "tensor(0.0027) tensor(530) tensor(635)\n",
            "tensor(0.0090) tensor(106) tensor(642)\n",
            "tensor(0.0083) tensor(47) tensor(622)\n",
            "tensor(0.0048) tensor(465) tensor(494)\n",
            "tensor(5.1221e-05) tensor(627) tensor(367)\n",
            "tensor(0.0017) tensor(479) tensor(631)\n",
            "tensor(0.0037) tensor(507) tensor(618)\n",
            "tensor(0.0014) tensor(643) tensor(591)\n",
            "tensor(0.0085) tensor(40) tensor(641)\n",
            "tensor(0.0043) tensor(537) tensor(284)\n",
            "tensor(0.0003) tensor(549) tensor(642)\n",
            "tensor(0.0082) tensor(622) tensor(213)\n",
            "tensor(0.0091) tensor(420) tensor(587)\n",
            "tensor(0.0077) tensor(631) tensor(190)\n",
            "tensor(0.0021) tensor(624) tensor(587)\n",
            "tensor(0.0023) tensor(635) tensor(331)\n",
            "tensor(0.0027) tensor(594) tensor(640)\n",
            "tensor(0.0010) tensor(285) tensor(642)\n",
            "tensor(0.0087) tensor(157) tensor(550)\n",
            "tensor(0.0009) tensor(69) tensor(642)\n",
            "tensor(0.0055) tensor(469) tensor(470)\n",
            "tensor(0.0039) tensor(47) tensor(644)\n",
            "tensor(4.1033e-06) tensor(406) tensor(627)\n",
            "tensor(0.0010) tensor(641) tensor(572)\n",
            "tensor(0.0015) tensor(643) tensor(453)\n",
            "tensor(0.0008) tensor(636) tensor(324)\n",
            "tensor(0.0062) tensor(473) tensor(530)\n",
            "tensor(0.0099) tensor(594) tensor(567)\n",
            "tensor(0.0046) tensor(317) tensor(630)\n",
            "tensor(0.0079) tensor(284) tensor(441)\n",
            "tensor(0.0073) tensor(582) tensor(623)\n",
            "tensor(0.0037) tensor(426) tensor(616)\n",
            "tensor(0.0082) tensor(572) tensor(438)\n",
            "tensor(0.0054) tensor(506) tensor(540)\n",
            "tensor(0.0074) tensor(331) tensor(551)\n",
            "tensor(0.0097) tensor(439) tensor(609)\n",
            "tensor(0.0095) tensor(466) tensor(596)\n",
            "tensor(0.0073) tensor(401) tensor(620)\n",
            "tensor(0.0021) tensor(587) tensor(306)\n",
            "tensor(0.0093) tensor(351) tensor(622)\n",
            "tensor(0.0056) tensor(626) tensor(601)\n",
            "tensor(0.0075) tensor(132) tensor(631)\n",
            "tensor(0.0030) tensor(630) tensor(576)\n",
            "tensor(0.0078) tensor(601) tensor(486)\n",
            "tensor(0.0046) tensor(382) tensor(628)\n",
            "tensor(0.0084) tensor(606) tensor(406)\n",
            "tensor(0.0037) tensor(439) tensor(630)\n",
            "tensor(0.0080) tensor(330) tensor(630)\n",
            "tensor(7.7095e-06) tensor(56) tensor(627)\n",
            "tensor(0.0039) tensor(93) tensor(306)\n",
            "tensor(0.0074) tensor(621) tensor(426)\n",
            "tensor(0.0007) tensor(644) tensor(577)\n",
            "tensor(0.0074) tensor(587) tensor(592)\n",
            "tensor(0.0017) tensor(202) tensor(642)\n",
            "tensor(0.0017) tensor(643) tensor(295)\n",
            "tensor(0.0054) tensor(418) tensor(540)\n",
            "tensor(0.0091) tensor(234) tensor(533)\n",
            "tensor(0.0081) tensor(559) tensor(624)\n",
            "tensor(0.0088) tensor(308) tensor(597)\n",
            "tensor(0.0024) tensor(618) tensor(632)\n",
            "tensor(0.0076) tensor(635) tensor(599)\n",
            "tensor(0.0011) tensor(636) tensor(551)\n",
            "tensor(0.0083) tensor(282) tensor(450)\n",
            "tensor(0.0064) tensor(405) tensor(561)\n",
            "tensor(0.0012) tensor(643) tensor(17)\n",
            "tensor(0.0023) tensor(644) tensor(512)\n",
            "tensor(0.0045) tensor(157) tensor(414)\n",
            "tensor(0.0041) tensor(612) tensor(630)\n",
            "tensor(0.0045) tensor(358) tensor(641)\n",
            "tensor(0.0087) tensor(601) tensor(553)\n",
            "tensor(0.0092) tensor(183) tensor(576)\n",
            "tensor(0.0063) tensor(586) tensor(570)\n",
            "tensor(0.0052) tensor(604) tensor(447)\n",
            "tensor(8.3994e-07) tensor(642) tensor(627)\n",
            "tensor(0.0099) tensor(578) tensor(331)\n",
            "tensor(0.0088) tensor(343) tensor(630)\n",
            "tensor(0.0085) tensor(497) tensor(613)\n",
            "tensor(0.0033) tensor(592) tensor(641)\n",
            "tensor(0.0072) tensor(638) tensor(604)\n",
            "tensor(0.0061) tensor(635) tensor(164)\n",
            "tensor(0.0044) tensor(596) tensor(93)\n",
            "tensor(9.7916e-07) tensor(414) tensor(627)\n",
            "tensor(0.0016) tensor(365) tensor(642)\n",
            "tensor(0.0051) tensor(628) tensor(384)\n",
            "tensor(0.0070) tensor(587) tensor(578)\n",
            "tensor(0.0035) tensor(636) tensor(543)\n",
            "tensor(0.0079) tensor(460) tensor(601)\n",
            "tensor(0.0017) tensor(622) tensor(306)\n",
            "tensor(0.0096) tensor(370) tensor(331)\n",
            "tensor(0.0074) tensor(512) tensor(64)\n",
            "tensor(0.0036) tensor(488) tensor(587)\n",
            "tensor(0.0048) tensor(644) tensor(103)\n",
            "tensor(2.4495e-05) tensor(627) tensor(465)\n",
            "tensor(0.0032) tensor(485) tensor(643)\n",
            "tensor(0.0063) tensor(401) tensor(625)\n",
            "tensor(0.0038) tensor(591) tensor(640)\n",
            "tensor(7.7821e-06) tensor(615) tensor(627)\n",
            "tensor(0.0069) tensor(600) tensor(625)\n",
            "tensor(0.0088) tensor(601) tensor(444)\n",
            "tensor(0.0072) tensor(588) tensor(541)\n",
            "tensor(7.0201e-06) tensor(44) tensor(627)\n",
            "tensor(0.0068) tensor(618) tensor(17)\n",
            "tensor(0.0050) tensor(603) tensor(561)\n",
            "tensor(0.0085) tensor(245) tensor(624)\n",
            "tensor(0.0025) tensor(618) tensor(596)\n",
            "tensor(0.0036) tensor(620) tensor(628)\n",
            "tensor(0.0045) tensor(348) tensor(494)\n",
            "tensor(0.0088) tensor(585) tensor(549)\n",
            "tensor(0.0026) tensor(401) tensor(641)\n",
            "tensor(4.2711e-06) tensor(263) tensor(627)\n",
            "tensor(0.0061) tensor(622) tensor(522)\n",
            "tensor(0.0012) tensor(631) tensor(240)\n",
            "tensor(0.0076) tensor(462) tensor(583)\n",
            "tensor(0.0084) tensor(348) tensor(504)\n",
            "tensor(1.3706e-05) tensor(627) tensor(596)\n",
            "tensor(0.0023) tensor(632) tensor(630)\n",
            "tensor(7.2284e-06) tensor(436) tensor(627)\n",
            "tensor(0.0009) tensor(549) tensor(636)\n",
            "tensor(0.0057) tensor(596) tensor(606)\n",
            "tensor(0.0051) tensor(564) tensor(640)\n",
            "tensor(0.0032) tensor(644) tensor(395)\n",
            "tensor(0.0072) tensor(441) tensor(509)\n",
            "tensor(0.0046) tensor(641) tensor(249)\n",
            "tensor(0.0098) tensor(115) tensor(635)\n",
            "tensor(0.0083) tensor(311) tensor(625)\n",
            "tensor(0.0034) tensor(635) tensor(183)\n",
            "tensor(0.0095) tensor(436) tensor(636)\n",
            "tensor(0.0070) tensor(542) tensor(635)\n",
            "tensor(0.0023) tensor(630) tensor(549)\n",
            "tensor(0.0009) tensor(587) tensor(641)\n",
            "tensor(0.0058) tensor(596) tensor(406)\n",
            "tensor(0.0031) tensor(571) tensor(348)\n",
            "tensor(0.0097) tensor(406) tensor(588)\n",
            "tensor(0.0086) tensor(182) tensor(465)\n",
            "tensor(0.0094) tensor(470) tensor(567)\n"
          ]
        }
      ],
      "source": [
        "c=0\n",
        "for i in lst:\n",
        "  j=i-12694\n",
        "  print(sig_scores[i],test_neg_u[j],test_neg_v[j])\n",
        "  for m,n in D.items():\n",
        "    if n==test_neg_v[j]:\n",
        "      #print(m)\n",
        "      break\n",
        "  c+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxEEEHk5_OaI"
      },
      "outputs": [],
      "source": [
        "for i,j in D.items():\n",
        "  if i=='CID000003198':\n",
        "    print(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEFLCweiN0n1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14284245-dee0-4548-a508-b100d56e7ae6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "test_neg_g.has_edges_between(363,290)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT4JhHONQYmb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "f47c7b47-97ac-44fe-c587-594efdd733e8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mat' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3825807861.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# where we are cross-checking between what we predicted and what is in the DrugBank (label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m493\u001b[0m  \u001b[0;32mand\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m639\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mat' is not defined"
          ]
        }
      ],
      "source": [
        "# where we are cross-checking between what we predicted and what is in the DrugBank (label)\n",
        "for i in range (len(mat)):\n",
        "  for j in range (len(mat)):\n",
        "    if i==493  and j==639:\n",
        "      print(mat[i][j])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 1. 重新从图 g 生成邻接矩阵 mat\n",
        "# g.adj() 获取稀疏矩阵，to_dense() 转为密集矩阵，numpy() 转为数组\n",
        "try:\n",
        "    # 新版 DGL 写法\n",
        "    mat = g.adj().to_dense().numpy()\n",
        "except:\n",
        "    # 旧版 DGL 写法 (或者如果上面报错)\n",
        "    import networkx as nx\n",
        "    g_nx = dgl.to_networkx(g)\n",
        "    mat = nx.to_numpy_array(g_nx)\n",
        "\n",
        "\n",
        "print(f\"矩阵形状: {mat.shape}\")\n",
        "\n",
        "# 2. 现在直接查询 (不要用你那个双重循环，太慢了)\n",
        "# 直接查第 493 行，第 639 列\n",
        "check_val = mat[493][639]\n",
        "print(f\"节点 493 和 639 之间的真实标签是: {check_val}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeTGYiMO6-jk",
        "outputId": "960ef725-d12a-49ad-f081-f11225ee078c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "矩阵形状: (645, 645)\n",
            "节点 493 和 639 之间的真实标签是: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV6KBq83O003"
      },
      "outputs": [],
      "source": [
        "for i in range(len(test_neg_u)):\n",
        "  if test_neg_u[i]==126 and test_neg_v[i]==338:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltOucGU3U8fd"
      },
      "outputs": [],
      "source": [
        "for i in range (len(test_pos_u)):\n",
        "  if test_pos_u[i]==656 and test_pos_v[i]==622:\n",
        "    print(sig_scores[i])\n",
        "  if test_pos_u[i]==622 and test_pos_v[i]==656:\n",
        "    print(sig_scores[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3wFelXBH5rQ"
      },
      "outputs": [],
      "source": [
        "********************************************************** GCN conv with ML **********************************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MFMLGBXM1F5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0e4a6cf1-3dc1-4f8b-c71f-4f71e6e43ab9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot compute fingerprint of empty list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-102519394.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mT1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mfunction1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mT1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot compute fingerprint of empty list"
          ]
        }
      ],
      "source": [
        "'''\n",
        "*************another approach\n",
        "Train, test (both positive and negative) graphs creation for training and testing purpose from the dgl graph (g)\n",
        "Source: DGL\n",
        "'''\n",
        "np.random.seed(42)\n",
        "# Split edge set for training and testing\n",
        "u, v = g.edges()\n",
        "eids = np.arange(g.number_of_edges())\n",
        "eids = np.random.permutation(eids)\n",
        "eids=eids.tolist()\n",
        "test_size = int(len(eids) * 0.8)\n",
        "train_size=len(eids)-(test_size)\n",
        "test_size1=int(test_size/2)\n",
        "\n",
        "\n",
        "T1=[]\n",
        "T2=[]\n",
        "numba.njit(target=\"cuda\")\n",
        "def func2(T1):\n",
        "  for i in eids:\n",
        "    m, n = int(u[i]), int(v[i])\n",
        "    c=(m,n)\n",
        "    T1.append(c)\n",
        "  return T1\n",
        "function1=jit(parallel=True) (func2)\n",
        "T1 = function1(T1)\n",
        "\n",
        "\n",
        "numba.njit(target=\"cuda\")\n",
        "def func3(T1,T2):\n",
        "  for i in T1:\n",
        "    m=i[0]\n",
        "    n=i[1]\n",
        "    c=(m,n)\n",
        "    d=(n,m)\n",
        "    if c not in T2 and d not in T2:\n",
        "      T2.append(c)\n",
        "    if len(T2)>=test_size1:\n",
        "      break\n",
        "  return T2\n",
        "function1=jit(parallel=True) (func3)\n",
        "T2 = function1(T1,T2)\n",
        "\n",
        "\n",
        "test_eids=[]\n",
        "numba.njit(target=\"cuda\")\n",
        "def func4(T2,test_eids,g):\n",
        "  for i in T2:\n",
        "    m= i[0]\n",
        "    n=i[1]\n",
        "    c=g.edge_id(m,n)\n",
        "    d=g.edge_id(n,m)\n",
        "    test_eids.append(c)\n",
        "    test_eids.append(d)\n",
        "  return test_eids\n",
        "function1=jit(parallel=True) (func4)\n",
        "test_eids = function1(T2,test_eids,g)\n",
        "\n",
        "\n",
        "train_eids=[]\n",
        "numba.njit(target=\"cuda\")\n",
        "def func6(eids,test_eids,train_eids):\n",
        "  i=0\n",
        "  for j in eids:\n",
        "    a=int(eids[i])\n",
        "    if a not in test_eids  and len(train_eids)<train_size:\n",
        "      train_eids.append(a)\n",
        "    i=i+1\n",
        "  return train_eids\n",
        "function1=jit(parallel=True) (func6)\n",
        "train_eids = function1(eids,test_eids,train_eids)\n",
        "\n",
        "# main one. splitting into train and test set.\n",
        "test_pos_u, test_pos_v = u[test_eids], v[test_eids]\n",
        "train_pos_u, train_pos_v = u[train_eids], v[train_eids]\n",
        "\n",
        "\n",
        "\n",
        "# Find all negative edges and split them for training and testing\n",
        "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
        "adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
        "neg_u, neg_v = np.where(adj_neg != 0)\n",
        "\n",
        "neg_eids = np.random.choice(len(neg_u), g.number_of_edges(),replace=False)\n",
        "g_=nx.from_numpy_matrix(adj_neg)\n",
        "g_=dgl.from_networkx(g_)\n",
        "\n",
        "T1=[]\n",
        "T2=[]\n",
        "numba.njit(target=\"cuda\")\n",
        "def func2(T1):\n",
        "  for i in neg_eids:\n",
        "    m, n = int(neg_u[i]), int(neg_v[i])\n",
        "    c=(m,n)\n",
        "    T1.append(c)\n",
        "  return T1\n",
        "function1=jit(parallel=True) (func2)\n",
        "T1 = function1(T1)\n",
        "\n",
        "\n",
        "numba.njit(target=\"cuda\")\n",
        "def func3(T1,T2):\n",
        "  for i in T1:\n",
        "    m=i[0]\n",
        "    n=i[1]\n",
        "    c=(m,n)\n",
        "    d=(n,m)\n",
        "    if c not in T2 and d not in T2:\n",
        "      T2.append(c)\n",
        "    if len(T2)>=test_size1:\n",
        "      break\n",
        "  return T2\n",
        "function1=jit(parallel=True) (func3)\n",
        "T2 = function1(T1,T2)\n",
        "\n",
        "\n",
        "test_neg_eids=[]\n",
        "numba.njit(target=\"cuda\")\n",
        "def func4(T2,test_eids,g):\n",
        "  for i in T2:\n",
        "    m= i[0]\n",
        "    n=i[1]\n",
        "    c=g.edge_id(m,n)\n",
        "    d=g.edge_id(n,m)\n",
        "    test_neg_eids.append(c)\n",
        "    test_neg_eids.append(d)\n",
        "  return test_neg_eids\n",
        "function1=jit(parallel=True) (func4)\n",
        "test_neg_eids = function1(T2,test_neg_eids,g_)\n",
        "\n",
        "\n",
        "train_neg_eids=[]\n",
        "numba.njit(target=\"cuda\")\n",
        "def func6(neg_eids,test_neg_eids,train_neg_eids):\n",
        "  i=0\n",
        "  for j in neg_eids:\n",
        "    a=int(neg_eids[i])\n",
        "    if a not in test_neg_eids and len(train_neg_eids)<(train_size):\n",
        "      train_neg_eids.append(a)\n",
        "    i=i+1\n",
        "  return train_neg_eids\n",
        "function1=jit(parallel=True) (func6)\n",
        "train_neg_eids = function1(neg_eids,test_neg_eids,train_neg_eids)\n",
        "\n",
        "test_neg_u, test_neg_v = neg_u[test_neg_eids], neg_v[test_neg_eids]\n",
        "train_neg_u, train_neg_v = neg_u[train_neg_eids], neg_v[train_neg_eids]\n",
        "\n",
        "\n",
        "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())# this is just the edge_list\n",
        "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
        "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
        "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n",
        "\n",
        "print(\"Total number of edges: \",g.number_of_edges())\n",
        "print(\"num of train POSITIVE edges: \",train_pos_g.number_of_edges(),\", num of test POSITIVE edges: \",test_pos_g.number_of_edges())\n",
        "print(\"num of train NEGATIVE edges: \",train_neg_g.number_of_edges(),\", num of test NEGATIVE edges: \",test_neg_g.number_of_edges())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PCIgtyzM2x1"
      },
      "outputs": [],
      "source": [
        "g_for_baseline = dgl.remove_edges(g, test_eids)\n",
        "#dgl.save_graphs(\"drive/My Drive/Colab Notebooks/Hypergraph_my/a.bin\",g_for_baseline)\n",
        "g_for_baseline = dgl.add_self_loop(g_for_baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAkTpAjJzJ9W"
      },
      "outputs": [],
      "source": [
        "class MLPPredictor(nn.Module):\n",
        "    def __init__(self, h_feats):\n",
        "        super().__init__()\n",
        "    def apply_edges(self, edges):\n",
        "        h = torch.cat([edges.src['h'], edges.dst['h']], 1).squeeze(1)\n",
        "        return {'score': h.squeeze(1)}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            g.apply_edges(self.apply_edges)\n",
        "            return g.edata['score']\n",
        "\n",
        "decoder = MLPPredictor(128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjwnjV_2H-a_"
      },
      "outputs": [],
      "source": [
        "from dgl.nn import GraphConv\n",
        "# ----------- 2. create model -------------- #\n",
        "# build a two-layer GraphSAGE model\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, h_feats)\n",
        "        self.conv2 = GraphConv(h_feats, h_feats)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "model = GCN(LEN, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykcPsc-i2d9z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "74ac7450-674a-4247-c1df-50a4a54a57a7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.wrappers.scikit_learn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1447762044.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.wrappers.scikit_learn'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB8T5SvM3Zkb"
      },
      "outputs": [],
      "source": [
        "e_feat=torch.zeros(LEN,LEN)\n",
        "e_feat.fill_diagonal_(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELIEN_q3LHo-"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "when u want to use saved GCN embeddings\n",
        "'''\n",
        "h =np.loadtxt(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/afterTrainingEmbedsGAT.txt\")\n",
        "h=torch.tensor(h)\n",
        "h=h.double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imlq53oXzqRE"
      },
      "outputs": [],
      "source": [
        "h = model(g_for_baseline, e_feat)\n",
        "pos_score_train = decoder(train_pos_g, h)\n",
        "neg_score_train = decoder(train_neg_g, h)\n",
        "pos_score_test = decoder(test_pos_g, h)\n",
        "neg_score_test = decoder(test_neg_g, h)\n",
        "\n",
        "scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n",
        "labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n",
        "scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n",
        "labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN00S7FpODnJ"
      },
      "outputs": [],
      "source": [
        "print(\"LR\")\n",
        "lr = LogisticRegression(class_weight=\"balanced\")\n",
        "lr.fit(scores_train, labels_train)\n",
        "predictions = lr.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
        "\n",
        "print(\"NB\")\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(scores_train, labels_train)\n",
        "predictions = gnb.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
        "\n",
        "print(\"RF\")\n",
        "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "clf.fit(scores_train, labels_train)\n",
        "predictions = clf.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gGnogCKReMV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oivgp2CGWg7I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0o6fss5WhhG"
      },
      "outputs": [],
      "source": [
        "********************************************************* Graph SAGE with ML ********************************************8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf96mSJ4WhQd"
      },
      "outputs": [],
      "source": [
        "from dgl.nn import SAGEConv\n",
        "\n",
        "# ----------- 2. create model -------------- #\n",
        "# build a two-layer GraphSAGE model\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats, 'pool')\n",
        "        self.conv2 = SAGEConv(h_feats, h_feats, 'pool')\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "model = GraphSAGE(1706, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4lNziClA8I3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "when u want to use saved GCN embeddings\n",
        "'''\n",
        "h =np.loadtxt(\"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/afterTrainingEmbedsGS_DB.txt\")\n",
        "h=torch.tensor(h)\n",
        "h=h.double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9wKMe8-XRur"
      },
      "outputs": [],
      "source": [
        "#h = model(g_for_baseline, e_feat)\n",
        "pos_score_train = decoder(train_pos_g, h)\n",
        "neg_score_train = decoder(train_neg_g, h)\n",
        "pos_score_test = decoder(test_pos_g, h)\n",
        "neg_score_test = decoder(test_neg_g, h)\n",
        "\n",
        "scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n",
        "labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n",
        "scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n",
        "labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNGAxn1vXU8b"
      },
      "outputs": [],
      "source": [
        "print(\"LR\")\n",
        "lr = LogisticRegression(class_weight=\"balanced\")\n",
        "lr.fit(scores_train, labels_train)\n",
        "predictions = lr.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
        "\n",
        "print(\"NB\")\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(scores_train, labels_train)\n",
        "predictions = gnb.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
        "\n",
        "print(\"RF\")\n",
        "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "clf.fit(scores_train, labels_train)\n",
        "predictions = clf.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxLv9xd4bFV8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WHBGPmHbGF8"
      },
      "outputs": [],
      "source": [
        "******************************************************************** GAT with ML ******************************************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaJeGFxobFln"
      },
      "outputs": [],
      "source": [
        "from dgl.nn import GATConv\n",
        "\n",
        "# ----------- 2. create model -------------- #\n",
        "# build a two-layer GraphSAGE model\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(in_feats, h_feats, num_heads=1)\n",
        "        self.conv2 = GATConv(h_feats, h_feats,num_heads=1)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "model = GAT(LEN, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wg1jBydbl1S"
      },
      "outputs": [],
      "source": [
        "h = model(g_for_baseline, e_feat)\n",
        "h=h.reshape(LEN,128)\n",
        "pos_score_train = decoder(train_pos_g, h)\n",
        "neg_score_train = decoder(train_neg_g, h)\n",
        "pos_score_test = decoder(test_pos_g, h)\n",
        "neg_score_test = decoder(test_neg_g, h)\n",
        "\n",
        "scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n",
        "labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n",
        "scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n",
        "labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcxRNSgibf08"
      },
      "outputs": [],
      "source": [
        "print(\"LR\")\n",
        "lr = LogisticRegression(class_weight=\"balanced\")\n",
        "lr.fit(scores_train, labels_train)\n",
        "predictions = lr.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
        "\n",
        "print(\"NB\")\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(scores_train, labels_train)\n",
        "predictions = gnb.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
        "\n",
        "print(\"RF\")\n",
        "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "clf.fit(scores_train, labels_train)\n",
        "predictions = clf.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbQB0DiBxK9C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXZ_MHWbzRCw"
      },
      "outputs": [],
      "source": [
        "***************************************************************** DeepWalk **********************************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0jc4LGYt5UI"
      },
      "outputs": [],
      "source": [
        "g_for_baseline_adj_matrix=g_for_baseline.adjacency_matrix_scipy()\n",
        "G_baseline = nx.from_numpy_array(g_for_baseline_adj_matrix,create_using=nx.DiGraph)\n",
        "nx.write_edgelist(G_baseline, \"/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/G_baseline_DB.txt\", data=False)\n",
        "\n",
        "# train model and generate embedding\n",
        "model = DeepWalk(walk_length=100, dimensions=128, window_size=5)\n",
        "model.fit(G_baseline)\n",
        "h = model.get_embedding()\n",
        "h=torch.from_numpy(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXqloGE1n7tQ"
      },
      "outputs": [],
      "source": [
        "pos_score_train = decoder(train_pos_g, h)\n",
        "neg_score_train = decoder(train_neg_g, h)\n",
        "pos_score_test = decoder(test_pos_g, h)\n",
        "neg_score_test = decoder(test_neg_g, h)\n",
        "\n",
        "scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n",
        "labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n",
        "scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n",
        "labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2l0fnbN0oDw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "8fb5ea89-021f-47e0-f449-d1b737f48b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'scores_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2670201162.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scores_train' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"LR\")\n",
        "lr = LogisticRegression(class_weight=\"balanced\")\n",
        "lr.fit(scores_train, labels_train)\n",
        "predictions = lr.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
        "\n",
        "print(\"NB\")\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(scores_train, labels_train)\n",
        "predictions = gnb.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
        "\n",
        "print(\"RF\")\n",
        "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "clf.fit(scores_train, labels_train)\n",
        "predictions = clf.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwHYtgNexkhO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dStwQAqB4WX3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0ENKLDg4R72"
      },
      "outputs": [],
      "source": [
        "******************************************************************* Node2Vec ***********************************8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8b2TBKC4YWT"
      },
      "outputs": [],
      "source": [
        "h=np.loadtxt('/content/drive/MyDrive/My Drive/Colab Notebooks/Hypergraph_my/data/node_2vec_embeddings_DB.txt',delimiter=',')\n",
        "h=torch.from_numpy(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVfHtc63XuZa"
      },
      "outputs": [],
      "source": [
        "pos_score_train = decoder(train_pos_g, h)\n",
        "neg_score_train = decoder(train_neg_g, h)\n",
        "pos_score_test = decoder(test_pos_g, h)\n",
        "neg_score_test = decoder(test_neg_g, h)\n",
        "\n",
        "scores_train = torch.cat([pos_score_train, neg_score_train]).detach().numpy()\n",
        "labels_train = torch.cat([torch.ones(pos_score_train.shape[0]), torch.zeros(neg_score_train.shape[0])]).detach().numpy()\n",
        "scores_test = torch.cat([pos_score_test, neg_score_test]).detach().numpy()\n",
        "labels_test = torch.cat([torch.ones(pos_score_test.shape[0]), torch.zeros(neg_score_test.shape[0])]).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6xQI7lWXw7M"
      },
      "outputs": [],
      "source": [
        "print(\"LR\")\n",
        "lr = LogisticRegression(class_weight=\"balanced\")\n",
        "lr.fit(scores_train, labels_train)\n",
        "predictions = lr.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
        "\n",
        "print(\"NB\")\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(scores_train, labels_train)\n",
        "predictions = gnb.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n",
        "\n",
        "print(\"RF\")\n",
        "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "clf.fit(scores_train, labels_train)\n",
        "predictions = clf.predict(scores_test)\n",
        "precision, recall, thresholds = precision_recall_curve(labels_test, predictions)\n",
        "auc_precision_recall = auc(recall, precision)\n",
        "print(\"accuracy: \",accuracy_score(labels_test,predictions),\"precision: \",precision_score(labels_test,predictions,average='weighted'),'recall: ',recall_score(labels_test,predictions,average='weighted'),'f1-score: ',f1_score(labels_test,predictions,average='weighted'),'ROC-AUC: ',roc_auc_score(labels_test, predictions),'PR-AUC: ',auc_precision_recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szIJQZdhYRnH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}